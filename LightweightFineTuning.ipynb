{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "This project is to classify emotions using a foundation model (GPT2). The purpose is to compare before and after light weight fine-tuning, how the model performances. \n",
    "\n",
    "\n",
    "here are the choices for PEFT techique, foundation model used, evaluation approach and the dataset for fine-tuning:\n",
    "\n",
    "* PEFT technique: Lora techique\n",
    "* Model: GPT-2\n",
    "* Evaluation approach: Classification evaluation approaches such as accuracy, confusion matrix, auc, f1 score, precision-recall curve\n",
    "* Fine-tuning dataset: zeroshot/twitter-financial-news-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c32f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e0f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "from peft import PeftModel, LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "from peft import AutoPeftModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce10d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "# Evaluate the predictions \n",
    "def classification_scores(model_name, y_true, y_pred):\n",
    "    \n",
    "    accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return pd.DataFrame({'Balanced Accuracy': np.round(accuracy, 3), \n",
    "                         'Precision': np.round(precision, 3), \n",
    "                         'Recall': np.round(recall, 3),\n",
    "                         'F1': np.round(f1, 3)}, \n",
    "                        index=[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8b18c",
   "metadata": {},
   "source": [
    "## Load the dataset zeroshot/twitter-financial-news-sentiment from datasets \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa96b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9543\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2388\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"zeroshot/twitter-financial-news-sentiment\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da832576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1442), (1, 1923), (2, 6178)]\n"
     ]
    }
   ],
   "source": [
    "# number of labels\n",
    "counts = Counter(dataset[\"train\"][\"label\"])\n",
    "sorted_counts = sorted(counts.items())\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, text=$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT\n",
      "label=0, text=$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3\n",
      "label=0, text=$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb\n",
      "label=0, text=$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N\n",
      "label=0, text=$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB\n",
      "label=0, text=$FTI - TechnipFMC downgraded at Berenberg but called Top Pick at Deutsche Bank https://t.co/XKcPDilIuU\n"
     ]
    }
   ],
   "source": [
    "for entry in dataset[\"train\"].select(range(6)):\n",
    "    text = entry[\"text\"]\n",
    "    label = entry[\"label\"]\n",
    "    print(f\"label={label}, text={text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b18e6c",
   "metadata": {},
   "source": [
    "## load GPT-2 tokenizer and tokenize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 9543\n",
      "}), 'validation': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2388\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load GPT-2 and tokenizer and evaluate on the test set\n",
    "\n",
    "# model name\n",
    "model_name = 'gpt2'\n",
    "number_labels = 3\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "def tokenize_datasets():\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenized_dataset = {}\n",
    "    splits = [\"train\", \"validation\"]\n",
    "\n",
    "    for split in splits:\n",
    "        tokenized_dataset[split] = dataset[split].map(tokenize_function, batched=True)\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenize_datasets()\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543b361",
   "metadata": {},
   "source": [
    "## Load and Setup the model, Train the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7837738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ebc0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a compute metrics function using scikit-learn\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model_name, number_labels,tokenized_dataset, requires_grad = False,  train=False, lora=False):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=number_labels,\n",
    "        id2label={0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"},\n",
    "        label2id={\"Bearish\": 0, \"Bullish\": 1, \"Neutral\": 2 }\n",
    "    )\n",
    "\n",
    "    # Freeze all the parameter of the base model\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "    # Ensure the model's config recognizes the padding token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # Use the HuggingFace Trainer class to handle the training and eval loop \n",
    "\n",
    "    # Check if MPS is available and use it if possible\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    if lora:\n",
    "        # Define LoRA configuration without the dropout parameter\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,  # Low-rank dimension\n",
    "            lora_alpha=16,  # Scaling factor\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"attn.c_attn\", \"attn.c_proj\"],  # Adjust the target modules to match GPT-2's architecture\n",
    "            inference_mode=False\n",
    "        )\n",
    "\n",
    "        # Create PEFT model with LoRA\n",
    "\n",
    "\n",
    "        peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "        peft_model.print_trainable_parameters()\n",
    "\n",
    "        model = peft_model\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./output\",\n",
    "            learning_rate=2e-3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_gpu_eval_batch_size=4,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if train:\n",
    "        print(trainer.train())   \n",
    "\n",
    "    print(trainer.evaluate())\n",
    "    \n",
    "    if lora:\n",
    "        # Save the PEFT weights and tokenizer\n",
    "\n",
    "        model.save_pretrained(\"gpt2-lora\")\n",
    "        tokenizer.save_pretrained(\"gpt2-lora-tokenizer\")\n",
    "\n",
    "\n",
    "\n",
    "    predictions = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def classification_performance(model_name, tokenized_dataset, predictions):\n",
    "\n",
    "    df = pd.DataFrame(tokenized_dataset[\"validation\"])\n",
    "\n",
    "    df = df[[\"text\", \"label\"]]\n",
    "\n",
    "    # add model predictions to the dataframe\n",
    "\n",
    "    df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "    return classification_scores(model_name, y_true=df[\"label\"], y_pred=df[\"predicted_label\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a791152",
   "metadata": {},
   "source": [
    "## Performance without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3152c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.424674034118652, 'eval_accuracy': 0.14530988274706869, 'eval_precision': 0.18159183102531826, 'eval_recall': 0.33183781681578894, 'eval_f1': 0.08509780056565543, 'eval_runtime': 219.5365, 'eval_samples_per_second': 10.877, 'eval_steps_per_second': 2.719}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_wo_train = train_model(model_name, number_labels,tokenized_dataset,  requires_grad = False, train=False, lora=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37995408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-2-wo-train</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Balanced Accuracy  Precision  Recall     F1\n",
       "gpt-2-wo-train              0.332      0.182   0.332  0.085"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the results\n",
    "\n",
    "classification_performance('gpt-2-wo-train', tokenized_dataset, predictions_wo_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d0be8",
   "metadata": {},
   "source": [
    "## Performance with only training the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2dd41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2386' max='2386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2386/2386 20:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.781600</td>\n",
       "      <td>0.696731</td>\n",
       "      <td>0.724037</td>\n",
       "      <td>0.635429</td>\n",
       "      <td>0.546451</td>\n",
       "      <td>0.571216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=2386, training_loss=0.8927352278550332, metrics={'train_runtime': 1207.2106, 'train_samples_per_second': 7.905, 'train_steps_per_second': 1.976, 'total_flos': 4987154811322368.0, 'train_loss': 0.8927352278550332, 'epoch': 1.0})\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6967308521270752, 'eval_accuracy': 0.724036850921273, 'eval_precision': 0.63542882758808, 'eval_recall': 0.5464511188350301, 'eval_f1': 0.5712162022952477, 'eval_runtime': 227.5547, 'eval_samples_per_second': 10.494, 'eval_steps_per_second': 2.624, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "predictions_cls_head = train_model(model_name, number_labels,tokenized_dataset,  requires_grad = False, train=True, lora=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fce4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt2-classification-head</th>\n",
       "      <td>0.546</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Balanced Accuracy  Precision  Recall     F1\n",
       "gpt2-classification-head              0.546      0.635   0.546  0.571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_performance('gpt2-classification-head', tokenized_dataset, predictions_cls_head)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1d7e1",
   "metadata": {},
   "source": [
    "## Fine-tuning the gpt2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f265d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2386' max='2386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2386/2386 45:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.728900</td>\n",
       "      <td>0.692716</td>\n",
       "      <td>0.729899</td>\n",
       "      <td>0.434830</td>\n",
       "      <td>0.524523</td>\n",
       "      <td>0.474179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Checkpoint destination directory ./output/checkpoint-2386 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=2386, training_loss=0.8360278120784328, metrics={'train_runtime': 2748.8817, 'train_samples_per_second': 3.472, 'train_steps_per_second': 0.868, 'total_flos': 4987154811322368.0, 'train_loss': 0.8360278120784328, 'epoch': 1.0})\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6927160620689392, 'eval_accuracy': 0.7298994974874372, 'eval_precision': 0.43482956810582357, 'eval_recall': 0.5245228652730165, 'eval_f1': 0.4741786105363874, 'eval_runtime': 224.3259, 'eval_samples_per_second': 10.645, 'eval_steps_per_second': 2.661, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "predictions_fine_tune = train_model(model_name, number_labels,tokenized_dataset,  requires_grad = True, train=True, lora=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3322de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt2-fine-tune</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Balanced Accuracy  Precision  Recall     F1\n",
       "gpt2-fine-tune              0.525      0.435   0.525  0.474"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_performance('gpt2-fine-tune', tokenized_dataset, predictions_fine_tune)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed6d5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b531e86b144374b65a4fee2700ed42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 446,976 || all params: 124,886,784 || trainable%: 0.3579049645477299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2386' max='2386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2386/2386 39:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.853370</td>\n",
       "      <td>0.655779</td>\n",
       "      <td>0.218593</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Checkpoint destination directory ./output/checkpoint-2386 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=2386, training_loss=0.9605849746486667, metrics={'train_runtime': 2369.9788, 'train_samples_per_second': 4.027, 'train_steps_per_second': 1.007, 'total_flos': 5013226905403392.0, 'train_loss': 0.9605849746486667, 'epoch': 1.0})\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8533703088760376, 'eval_accuracy': 0.6557788944723618, 'eval_precision': 0.2185929648241206, 'eval_recall': 0.3333333333333333, 'eval_f1': 0.26403641881638845, 'eval_runtime': 235.1204, 'eval_samples_per_second': 10.156, 'eval_steps_per_second': 2.539, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "predictions_peft = train_model(model_name, number_labels,tokenized_dataset,  requires_grad = False, train=True, lora=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9c2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt2-peft</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Balanced Accuracy  Precision  Recall     F1\n",
       "gpt2-peft              0.333      0.219   0.333  0.264"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_performance('gpt2-peft', tokenized_dataset, predictions_peft) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need to save the config?\n",
    "# config = peft_model.config\n",
    "# config.save_pretrained('gpt2-lora-config')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],  # Adjust the target modules to match GPT-2's architecture\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "# lora_model = AutoPeftModelForSequenceClassification.from_pretrained(model_name, config=peft_config, num_labels=number_labels)\n",
    "\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(\"gpt2-lora-tokenizer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e095ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finised loading PEFT pretrained model!\n"
     ]
    }
   ],
   "source": [
    "# Loading PEFT pretrained model\n",
    "peft_model_id = 'gpt2-lora'\n",
    "print(\"Loading PEFT pretrained model...\")\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=peft_model_id,\n",
    "    config=peft_config,\n",
    "    num_labels=number_labels\n",
    "    )\n",
    "# these are for GPT model: since we have custom padding token we need to initialise it for the model\n",
    "# resize model embedding to match new tokenizer\n",
    "inference_model.resize_token_embeddings(len(tokenizer_lora))\n",
    "# fix model padding token id\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id\n",
    "\n",
    "inference_model.to(device)\n",
    "inference_model.eval()\n",
    "print(\"Finised loading PEFT pretrained model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0499d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[13578,   318,  8066,   307,  1029]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"stock is gonna be high\"\n",
    "\n",
    "tokenizer_lora.pad_token = tokenizer_lora.eos_token\n",
    "\n",
    "inputs = tokenizer_lora(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78aa13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37eee0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[13578,   318,  8066,   307,  1029]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set the model to evaluation mode to disable dropout, batch norm, etc.\n",
    "inference_model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = inference_model(**inputs)\n",
    "\n",
    "# Predict labels by taking the argmax over the logits\n",
    "pred_labels = torch.argmax(torch.nn.functional.softmax(outputs.logits, dim=-1), dim=-1, keepdim=False)\n",
    "\n",
    "# # Append true and predicted labels to their respective tensors\n",
    "# true_labels = torch.cat((true_labels, batch['labels'].detach().cpu()))\n",
    "# predicted_labels = torch.cat((predicted_labels, pred_labels.detach().cpu()))\n",
    "\n",
    "# # Store true and predicted labels in a dictionary and return\n",
    "# outputs = dict()\n",
    "# outputs['true_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5157f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2388/2388 [00:01<00:00, 2163.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted label: 2\n",
      "Sample 1: Predicted label: 1\n",
      "Sample 2: Predicted label: 2\n",
      "Sample 3: Predicted label: 1\n",
      "Sample 4: Predicted label: 2\n",
      "Sample 5: Predicted label: 2\n",
      "Sample 6: Predicted label: 2\n",
      "Sample 7: Predicted label: 2\n",
      "Sample 8: Predicted label: 2\n",
      "Sample 9: Predicted label: 0\n",
      "Sample 10: Predicted label: 1\n",
      "Sample 11: Predicted label: 1\n",
      "Sample 12: Predicted label: 2\n",
      "Sample 13: Predicted label: 1\n",
      "Sample 14: Predicted label: 1\n",
      "Sample 15: Predicted label: 1\n",
      "Sample 16: Predicted label: 1\n",
      "Sample 17: Predicted label: 0\n",
      "Sample 18: Predicted label: 0\n",
      "Sample 19: Predicted label: 0\n",
      "Sample 20: Predicted label: 0\n",
      "Sample 21: Predicted label: 0\n",
      "Sample 22: Predicted label: 1\n",
      "Sample 23: Predicted label: 1\n",
      "Sample 24: Predicted label: 0\n",
      "Sample 25: Predicted label: 1\n",
      "Sample 26: Predicted label: 0\n",
      "Sample 27: Predicted label: 1\n",
      "Sample 28: Predicted label: 1\n",
      "Sample 29: Predicted label: 1\n",
      "Sample 30: Predicted label: 1\n",
      "Sample 31: Predicted label: 2\n",
      "Sample 32: Predicted label: 1\n",
      "Sample 33: Predicted label: 2\n",
      "Sample 34: Predicted label: 1\n",
      "Sample 35: Predicted label: 2\n",
      "Sample 36: Predicted label: 2\n",
      "Sample 37: Predicted label: 1\n",
      "Sample 38: Predicted label: 2\n",
      "Sample 39: Predicted label: 2\n",
      "Sample 40: Predicted label: 2\n",
      "Sample 41: Predicted label: 1\n",
      "Sample 42: Predicted label: 2\n",
      "Sample 43: Predicted label: 1\n",
      "Sample 44: Predicted label: 1\n",
      "Sample 45: Predicted label: 0\n",
      "Sample 46: Predicted label: 1\n",
      "Sample 47: Predicted label: 1\n",
      "Sample 48: Predicted label: 1\n",
      "Sample 49: Predicted label: 2\n",
      "Sample 50: Predicted label: 1\n",
      "Sample 51: Predicted label: 1\n",
      "Sample 52: Predicted label: 1\n",
      "Sample 53: Predicted label: 1\n",
      "Sample 54: Predicted label: 1\n",
      "Sample 55: Predicted label: 0\n",
      "Sample 56: Predicted label: 2\n",
      "Sample 57: Predicted label: 1\n",
      "Sample 58: Predicted label: 1\n",
      "Sample 59: Predicted label: 1\n",
      "Sample 60: Predicted label: 1\n",
      "Sample 61: Predicted label: 2\n",
      "Sample 62: Predicted label: 1\n",
      "Sample 63: Predicted label: 1\n",
      "Sample 64: Predicted label: 1\n",
      "Sample 65: Predicted label: 1\n",
      "Sample 66: Predicted label: 1\n",
      "Sample 67: Predicted label: 2\n",
      "Sample 68: Predicted label: 1\n",
      "Sample 69: Predicted label: 1\n",
      "Sample 70: Predicted label: 0\n",
      "Sample 71: Predicted label: 1\n",
      "Sample 72: Predicted label: 2\n",
      "Sample 73: Predicted label: 1\n",
      "Sample 74: Predicted label: 2\n",
      "Sample 75: Predicted label: 0\n",
      "Sample 76: Predicted label: 1\n",
      "Sample 77: Predicted label: 1\n",
      "Sample 78: Predicted label: 0\n",
      "Sample 79: Predicted label: 1\n",
      "Sample 80: Predicted label: 1\n",
      "Sample 81: Predicted label: 1\n",
      "Sample 82: Predicted label: 1\n",
      "Sample 83: Predicted label: 0\n",
      "Sample 84: Predicted label: 1\n",
      "Sample 85: Predicted label: 2\n",
      "Sample 86: Predicted label: 1\n",
      "Sample 87: Predicted label: 1\n",
      "Sample 88: Predicted label: 1\n",
      "Sample 89: Predicted label: 1\n",
      "Sample 90: Predicted label: 1\n",
      "Sample 91: Predicted label: 1\n",
      "Sample 92: Predicted label: 2\n",
      "Sample 93: Predicted label: 2\n",
      "Sample 94: Predicted label: 1\n",
      "Sample 95: Predicted label: 2\n",
      "Sample 96: Predicted label: 2\n",
      "Sample 97: Predicted label: 1\n",
      "Sample 98: Predicted label: 2\n",
      "Sample 99: Predicted label: 1\n",
      "Sample 100: Predicted label: 2\n",
      "Sample 101: Predicted label: 2\n",
      "Sample 102: Predicted label: 1\n",
      "Sample 103: Predicted label: 2\n",
      "Sample 104: Predicted label: 1\n",
      "Sample 105: Predicted label: 1\n",
      "Sample 106: Predicted label: 2\n",
      "Sample 107: Predicted label: 2\n",
      "Sample 108: Predicted label: 0\n",
      "Sample 109: Predicted label: 2\n",
      "Sample 110: Predicted label: 2\n",
      "Sample 111: Predicted label: 2\n",
      "Sample 112: Predicted label: 2\n",
      "Sample 113: Predicted label: 2\n",
      "Sample 114: Predicted label: 2\n",
      "Sample 115: Predicted label: 2\n",
      "Sample 116: Predicted label: 2\n",
      "Sample 117: Predicted label: 2\n",
      "Sample 118: Predicted label: 2\n",
      "Sample 119: Predicted label: 2\n",
      "Sample 120: Predicted label: 2\n",
      "Sample 121: Predicted label: 2\n",
      "Sample 122: Predicted label: 2\n",
      "Sample 123: Predicted label: 2\n",
      "Sample 124: Predicted label: 2\n",
      "Sample 125: Predicted label: 2\n",
      "Sample 126: Predicted label: 2\n",
      "Sample 127: Predicted label: 2\n",
      "Sample 128: Predicted label: 2\n",
      "Sample 129: Predicted label: 2\n",
      "Sample 130: Predicted label: 2\n",
      "Sample 131: Predicted label: 2\n",
      "Sample 132: Predicted label: 2\n",
      "Sample 133: Predicted label: 2\n",
      "Sample 134: Predicted label: 2\n",
      "Sample 135: Predicted label: 2\n",
      "Sample 136: Predicted label: 2\n",
      "Sample 137: Predicted label: 0\n",
      "Sample 138: Predicted label: 2\n",
      "Sample 139: Predicted label: 2\n",
      "Sample 140: Predicted label: 2\n",
      "Sample 141: Predicted label: 2\n",
      "Sample 142: Predicted label: 2\n",
      "Sample 143: Predicted label: 0\n",
      "Sample 144: Predicted label: 2\n",
      "Sample 145: Predicted label: 2\n",
      "Sample 146: Predicted label: 2\n",
      "Sample 147: Predicted label: 2\n",
      "Sample 148: Predicted label: 2\n",
      "Sample 149: Predicted label: 2\n",
      "Sample 150: Predicted label: 2\n",
      "Sample 151: Predicted label: 2\n",
      "Sample 152: Predicted label: 2\n",
      "Sample 153: Predicted label: 2\n",
      "Sample 154: Predicted label: 0\n",
      "Sample 155: Predicted label: 2\n",
      "Sample 156: Predicted label: 2\n",
      "Sample 157: Predicted label: 2\n",
      "Sample 158: Predicted label: 2\n",
      "Sample 159: Predicted label: 2\n",
      "Sample 160: Predicted label: 2\n",
      "Sample 161: Predicted label: 2\n",
      "Sample 162: Predicted label: 0\n",
      "Sample 163: Predicted label: 2\n",
      "Sample 164: Predicted label: 2\n",
      "Sample 165: Predicted label: 2\n",
      "Sample 166: Predicted label: 2\n",
      "Sample 167: Predicted label: 2\n",
      "Sample 168: Predicted label: 2\n",
      "Sample 169: Predicted label: 2\n",
      "Sample 170: Predicted label: 2\n",
      "Sample 171: Predicted label: 2\n",
      "Sample 172: Predicted label: 2\n",
      "Sample 173: Predicted label: 2\n",
      "Sample 174: Predicted label: 2\n",
      "Sample 175: Predicted label: 2\n",
      "Sample 176: Predicted label: 2\n",
      "Sample 177: Predicted label: 0\n",
      "Sample 178: Predicted label: 2\n",
      "Sample 179: Predicted label: 2\n",
      "Sample 180: Predicted label: 2\n",
      "Sample 181: Predicted label: 2\n",
      "Sample 182: Predicted label: 2\n",
      "Sample 183: Predicted label: 0\n",
      "Sample 184: Predicted label: 2\n",
      "Sample 185: Predicted label: 2\n",
      "Sample 186: Predicted label: 2\n",
      "Sample 187: Predicted label: 0\n",
      "Sample 188: Predicted label: 2\n",
      "Sample 189: Predicted label: 2\n",
      "Sample 190: Predicted label: 0\n",
      "Sample 191: Predicted label: 2\n",
      "Sample 192: Predicted label: 0\n",
      "Sample 193: Predicted label: 2\n",
      "Sample 194: Predicted label: 0\n",
      "Sample 195: Predicted label: 0\n",
      "Sample 196: Predicted label: 1\n",
      "Sample 197: Predicted label: 2\n",
      "Sample 198: Predicted label: 1\n",
      "Sample 199: Predicted label: 2\n",
      "Sample 200: Predicted label: 2\n",
      "Sample 201: Predicted label: 2\n",
      "Sample 202: Predicted label: 2\n",
      "Sample 203: Predicted label: 2\n",
      "Sample 204: Predicted label: 2\n",
      "Sample 205: Predicted label: 2\n",
      "Sample 206: Predicted label: 2\n",
      "Sample 207: Predicted label: 2\n",
      "Sample 208: Predicted label: 2\n",
      "Sample 209: Predicted label: 2\n",
      "Sample 210: Predicted label: 2\n",
      "Sample 211: Predicted label: 2\n",
      "Sample 212: Predicted label: 2\n",
      "Sample 213: Predicted label: 2\n",
      "Sample 214: Predicted label: 1\n",
      "Sample 215: Predicted label: 1\n",
      "Sample 216: Predicted label: 2\n",
      "Sample 217: Predicted label: 2\n",
      "Sample 218: Predicted label: 1\n",
      "Sample 219: Predicted label: 2\n",
      "Sample 220: Predicted label: 2\n",
      "Sample 221: Predicted label: 1\n",
      "Sample 222: Predicted label: 2\n",
      "Sample 223: Predicted label: 2\n",
      "Sample 224: Predicted label: 2\n",
      "Sample 225: Predicted label: 1\n",
      "Sample 226: Predicted label: 2\n",
      "Sample 227: Predicted label: 2\n",
      "Sample 228: Predicted label: 2\n",
      "Sample 229: Predicted label: 2\n",
      "Sample 230: Predicted label: 2\n",
      "Sample 231: Predicted label: 2\n",
      "Sample 232: Predicted label: 2\n",
      "Sample 233: Predicted label: 2\n",
      "Sample 234: Predicted label: 2\n",
      "Sample 235: Predicted label: 2\n",
      "Sample 236: Predicted label: 2\n",
      "Sample 237: Predicted label: 1\n",
      "Sample 238: Predicted label: 2\n",
      "Sample 239: Predicted label: 1\n",
      "Sample 240: Predicted label: 1\n",
      "Sample 241: Predicted label: 2\n",
      "Sample 242: Predicted label: 2\n",
      "Sample 243: Predicted label: 0\n",
      "Sample 244: Predicted label: 2\n",
      "Sample 245: Predicted label: 2\n",
      "Sample 246: Predicted label: 1\n",
      "Sample 247: Predicted label: 2\n",
      "Sample 248: Predicted label: 2\n",
      "Sample 249: Predicted label: 2\n",
      "Sample 250: Predicted label: 2\n",
      "Sample 251: Predicted label: 2\n",
      "Sample 252: Predicted label: 2\n",
      "Sample 253: Predicted label: 2\n",
      "Sample 254: Predicted label: 2\n",
      "Sample 255: Predicted label: 2\n",
      "Sample 256: Predicted label: 2\n",
      "Sample 257: Predicted label: 2\n",
      "Sample 258: Predicted label: 2\n",
      "Sample 259: Predicted label: 2\n",
      "Sample 260: Predicted label: 2\n",
      "Sample 261: Predicted label: 2\n",
      "Sample 262: Predicted label: 2\n",
      "Sample 263: Predicted label: 2\n",
      "Sample 264: Predicted label: 2\n",
      "Sample 265: Predicted label: 2\n",
      "Sample 266: Predicted label: 2\n",
      "Sample 267: Predicted label: 2\n",
      "Sample 268: Predicted label: 2\n",
      "Sample 269: Predicted label: 2\n",
      "Sample 270: Predicted label: 2\n",
      "Sample 271: Predicted label: 2\n",
      "Sample 272: Predicted label: 2\n",
      "Sample 273: Predicted label: 2\n",
      "Sample 274: Predicted label: 2\n",
      "Sample 275: Predicted label: 2\n",
      "Sample 276: Predicted label: 2\n",
      "Sample 277: Predicted label: 2\n",
      "Sample 278: Predicted label: 2\n",
      "Sample 279: Predicted label: 2\n",
      "Sample 280: Predicted label: 2\n",
      "Sample 281: Predicted label: 2\n",
      "Sample 282: Predicted label: 2\n",
      "Sample 283: Predicted label: 2\n",
      "Sample 284: Predicted label: 2\n",
      "Sample 285: Predicted label: 2\n",
      "Sample 286: Predicted label: 2\n",
      "Sample 287: Predicted label: 2\n",
      "Sample 288: Predicted label: 2\n",
      "Sample 289: Predicted label: 2\n",
      "Sample 290: Predicted label: 1\n",
      "Sample 291: Predicted label: 2\n",
      "Sample 292: Predicted label: 2\n",
      "Sample 293: Predicted label: 2\n",
      "Sample 294: Predicted label: 2\n",
      "Sample 295: Predicted label: 2\n",
      "Sample 296: Predicted label: 2\n",
      "Sample 297: Predicted label: 2\n",
      "Sample 298: Predicted label: 2\n",
      "Sample 299: Predicted label: 2\n",
      "Sample 300: Predicted label: 2\n",
      "Sample 301: Predicted label: 2\n",
      "Sample 302: Predicted label: 2\n",
      "Sample 303: Predicted label: 2\n",
      "Sample 304: Predicted label: 2\n",
      "Sample 305: Predicted label: 2\n",
      "Sample 306: Predicted label: 2\n",
      "Sample 307: Predicted label: 2\n",
      "Sample 308: Predicted label: 2\n",
      "Sample 309: Predicted label: 2\n",
      "Sample 310: Predicted label: 2\n",
      "Sample 311: Predicted label: 2\n",
      "Sample 312: Predicted label: 1\n",
      "Sample 313: Predicted label: 2\n",
      "Sample 314: Predicted label: 2\n",
      "Sample 315: Predicted label: 2\n",
      "Sample 316: Predicted label: 2\n",
      "Sample 317: Predicted label: 2\n",
      "Sample 318: Predicted label: 2\n",
      "Sample 319: Predicted label: 1\n",
      "Sample 320: Predicted label: 2\n",
      "Sample 321: Predicted label: 2\n",
      "Sample 322: Predicted label: 2\n",
      "Sample 323: Predicted label: 2\n",
      "Sample 324: Predicted label: 2\n",
      "Sample 325: Predicted label: 2\n",
      "Sample 326: Predicted label: 2\n",
      "Sample 327: Predicted label: 2\n",
      "Sample 328: Predicted label: 2\n",
      "Sample 329: Predicted label: 2\n",
      "Sample 330: Predicted label: 1\n",
      "Sample 331: Predicted label: 2\n",
      "Sample 332: Predicted label: 2\n",
      "Sample 333: Predicted label: 2\n",
      "Sample 334: Predicted label: 2\n",
      "Sample 335: Predicted label: 2\n",
      "Sample 336: Predicted label: 2\n",
      "Sample 337: Predicted label: 2\n",
      "Sample 338: Predicted label: 2\n",
      "Sample 339: Predicted label: 2\n",
      "Sample 340: Predicted label: 2\n",
      "Sample 341: Predicted label: 2\n",
      "Sample 342: Predicted label: 2\n",
      "Sample 343: Predicted label: 2\n",
      "Sample 344: Predicted label: 2\n",
      "Sample 345: Predicted label: 2\n",
      "Sample 346: Predicted label: 2\n",
      "Sample 347: Predicted label: 2\n",
      "Sample 348: Predicted label: 2\n",
      "Sample 349: Predicted label: 2\n",
      "Sample 350: Predicted label: 2\n",
      "Sample 351: Predicted label: 2\n",
      "Sample 352: Predicted label: 2\n",
      "Sample 353: Predicted label: 2\n",
      "Sample 354: Predicted label: 2\n",
      "Sample 355: Predicted label: 2\n",
      "Sample 356: Predicted label: 2\n",
      "Sample 357: Predicted label: 2\n",
      "Sample 358: Predicted label: 2\n",
      "Sample 359: Predicted label: 2\n",
      "Sample 360: Predicted label: 2\n",
      "Sample 361: Predicted label: 2\n",
      "Sample 362: Predicted label: 2\n",
      "Sample 363: Predicted label: 2\n",
      "Sample 364: Predicted label: 2\n",
      "Sample 365: Predicted label: 2\n",
      "Sample 366: Predicted label: 2\n",
      "Sample 367: Predicted label: 2\n",
      "Sample 368: Predicted label: 2\n",
      "Sample 369: Predicted label: 2\n",
      "Sample 370: Predicted label: 2\n",
      "Sample 371: Predicted label: 2\n",
      "Sample 372: Predicted label: 2\n",
      "Sample 373: Predicted label: 1\n",
      "Sample 374: Predicted label: 2\n",
      "Sample 375: Predicted label: 2\n",
      "Sample 376: Predicted label: 2\n",
      "Sample 377: Predicted label: 2\n",
      "Sample 378: Predicted label: 2\n",
      "Sample 379: Predicted label: 2\n",
      "Sample 380: Predicted label: 2\n",
      "Sample 381: Predicted label: 2\n",
      "Sample 382: Predicted label: 2\n",
      "Sample 383: Predicted label: 1\n",
      "Sample 384: Predicted label: 2\n",
      "Sample 385: Predicted label: 0\n",
      "Sample 386: Predicted label: 2\n",
      "Sample 387: Predicted label: 2\n",
      "Sample 388: Predicted label: 2\n",
      "Sample 389: Predicted label: 2\n",
      "Sample 390: Predicted label: 2\n",
      "Sample 391: Predicted label: 2\n",
      "Sample 392: Predicted label: 0\n",
      "Sample 393: Predicted label: 2\n",
      "Sample 394: Predicted label: 2\n",
      "Sample 395: Predicted label: 2\n",
      "Sample 396: Predicted label: 2\n",
      "Sample 397: Predicted label: 2\n",
      "Sample 398: Predicted label: 2\n",
      "Sample 399: Predicted label: 2\n",
      "Sample 400: Predicted label: 2\n",
      "Sample 401: Predicted label: 2\n",
      "Sample 402: Predicted label: 2\n",
      "Sample 403: Predicted label: 2\n",
      "Sample 404: Predicted label: 2\n",
      "Sample 405: Predicted label: 2\n",
      "Sample 406: Predicted label: 2\n",
      "Sample 407: Predicted label: 2\n",
      "Sample 408: Predicted label: 0\n",
      "Sample 409: Predicted label: 1\n",
      "Sample 410: Predicted label: 2\n",
      "Sample 411: Predicted label: 2\n",
      "Sample 412: Predicted label: 2\n",
      "Sample 413: Predicted label: 2\n",
      "Sample 414: Predicted label: 2\n",
      "Sample 415: Predicted label: 2\n",
      "Sample 416: Predicted label: 2\n",
      "Sample 417: Predicted label: 2\n",
      "Sample 418: Predicted label: 0\n",
      "Sample 419: Predicted label: 2\n",
      "Sample 420: Predicted label: 1\n",
      "Sample 421: Predicted label: 2\n",
      "Sample 422: Predicted label: 2\n",
      "Sample 423: Predicted label: 2\n",
      "Sample 424: Predicted label: 1\n",
      "Sample 425: Predicted label: 2\n",
      "Sample 426: Predicted label: 2\n",
      "Sample 427: Predicted label: 2\n",
      "Sample 428: Predicted label: 2\n",
      "Sample 429: Predicted label: 2\n",
      "Sample 430: Predicted label: 2\n",
      "Sample 431: Predicted label: 2\n",
      "Sample 432: Predicted label: 2\n",
      "Sample 433: Predicted label: 2\n",
      "Sample 434: Predicted label: 2\n",
      "Sample 435: Predicted label: 2\n",
      "Sample 436: Predicted label: 2\n",
      "Sample 437: Predicted label: 2\n",
      "Sample 438: Predicted label: 2\n",
      "Sample 439: Predicted label: 2\n",
      "Sample 440: Predicted label: 2\n",
      "Sample 441: Predicted label: 2\n",
      "Sample 442: Predicted label: 1\n",
      "Sample 443: Predicted label: 2\n",
      "Sample 444: Predicted label: 2\n",
      "Sample 445: Predicted label: 2\n",
      "Sample 446: Predicted label: 2\n",
      "Sample 447: Predicted label: 2\n",
      "Sample 448: Predicted label: 2\n",
      "Sample 449: Predicted label: 2\n",
      "Sample 450: Predicted label: 2\n",
      "Sample 451: Predicted label: 2\n",
      "Sample 452: Predicted label: 2\n",
      "Sample 453: Predicted label: 2\n",
      "Sample 454: Predicted label: 2\n",
      "Sample 455: Predicted label: 2\n",
      "Sample 456: Predicted label: 2\n",
      "Sample 457: Predicted label: 2\n",
      "Sample 458: Predicted label: 2\n",
      "Sample 459: Predicted label: 1\n",
      "Sample 460: Predicted label: 2\n",
      "Sample 461: Predicted label: 2\n",
      "Sample 462: Predicted label: 1\n",
      "Sample 463: Predicted label: 2\n",
      "Sample 464: Predicted label: 2\n",
      "Sample 465: Predicted label: 2\n",
      "Sample 466: Predicted label: 2\n",
      "Sample 467: Predicted label: 2\n",
      "Sample 468: Predicted label: 2\n",
      "Sample 469: Predicted label: 0\n",
      "Sample 470: Predicted label: 2\n",
      "Sample 471: Predicted label: 2\n",
      "Sample 472: Predicted label: 2\n",
      "Sample 473: Predicted label: 2\n",
      "Sample 474: Predicted label: 2\n",
      "Sample 475: Predicted label: 2\n",
      "Sample 476: Predicted label: 2\n",
      "Sample 477: Predicted label: 2\n",
      "Sample 478: Predicted label: 2\n",
      "Sample 479: Predicted label: 2\n",
      "Sample 480: Predicted label: 2\n",
      "Sample 481: Predicted label: 2\n",
      "Sample 482: Predicted label: 2\n",
      "Sample 483: Predicted label: 2\n",
      "Sample 484: Predicted label: 2\n",
      "Sample 485: Predicted label: 2\n",
      "Sample 486: Predicted label: 2\n",
      "Sample 487: Predicted label: 2\n",
      "Sample 488: Predicted label: 2\n",
      "Sample 489: Predicted label: 2\n",
      "Sample 490: Predicted label: 2\n",
      "Sample 491: Predicted label: 2\n",
      "Sample 492: Predicted label: 2\n",
      "Sample 493: Predicted label: 2\n",
      "Sample 494: Predicted label: 2\n",
      "Sample 495: Predicted label: 2\n",
      "Sample 496: Predicted label: 2\n",
      "Sample 497: Predicted label: 2\n",
      "Sample 498: Predicted label: 2\n",
      "Sample 499: Predicted label: 2\n",
      "Sample 500: Predicted label: 2\n",
      "Sample 501: Predicted label: 1\n",
      "Sample 502: Predicted label: 2\n",
      "Sample 503: Predicted label: 2\n",
      "Sample 504: Predicted label: 2\n",
      "Sample 505: Predicted label: 2\n",
      "Sample 506: Predicted label: 2\n",
      "Sample 507: Predicted label: 1\n",
      "Sample 508: Predicted label: 2\n",
      "Sample 509: Predicted label: 2\n",
      "Sample 510: Predicted label: 2\n",
      "Sample 511: Predicted label: 2\n",
      "Sample 512: Predicted label: 2\n",
      "Sample 513: Predicted label: 2\n",
      "Sample 514: Predicted label: 2\n",
      "Sample 515: Predicted label: 2\n",
      "Sample 516: Predicted label: 1\n",
      "Sample 517: Predicted label: 1\n",
      "Sample 518: Predicted label: 2\n",
      "Sample 519: Predicted label: 2\n",
      "Sample 520: Predicted label: 2\n",
      "Sample 521: Predicted label: 2\n",
      "Sample 522: Predicted label: 2\n",
      "Sample 523: Predicted label: 2\n",
      "Sample 524: Predicted label: 2\n",
      "Sample 525: Predicted label: 2\n",
      "Sample 526: Predicted label: 2\n",
      "Sample 527: Predicted label: 2\n",
      "Sample 528: Predicted label: 2\n",
      "Sample 529: Predicted label: 2\n",
      "Sample 530: Predicted label: 2\n",
      "Sample 531: Predicted label: 2\n",
      "Sample 532: Predicted label: 2\n",
      "Sample 533: Predicted label: 2\n",
      "Sample 534: Predicted label: 2\n",
      "Sample 535: Predicted label: 1\n",
      "Sample 536: Predicted label: 2\n",
      "Sample 537: Predicted label: 2\n",
      "Sample 538: Predicted label: 2\n",
      "Sample 539: Predicted label: 2\n",
      "Sample 540: Predicted label: 2\n",
      "Sample 541: Predicted label: 2\n",
      "Sample 542: Predicted label: 2\n",
      "Sample 543: Predicted label: 2\n",
      "Sample 544: Predicted label: 2\n",
      "Sample 545: Predicted label: 2\n",
      "Sample 546: Predicted label: 2\n",
      "Sample 547: Predicted label: 2\n",
      "Sample 548: Predicted label: 1\n",
      "Sample 549: Predicted label: 2\n",
      "Sample 550: Predicted label: 2\n",
      "Sample 551: Predicted label: 2\n",
      "Sample 552: Predicted label: 2\n",
      "Sample 553: Predicted label: 0\n",
      "Sample 554: Predicted label: 2\n",
      "Sample 555: Predicted label: 1\n",
      "Sample 556: Predicted label: 2\n",
      "Sample 557: Predicted label: 1\n",
      "Sample 558: Predicted label: 1\n",
      "Sample 559: Predicted label: 2\n",
      "Sample 560: Predicted label: 2\n",
      "Sample 561: Predicted label: 2\n",
      "Sample 562: Predicted label: 2\n",
      "Sample 563: Predicted label: 2\n",
      "Sample 564: Predicted label: 2\n",
      "Sample 565: Predicted label: 2\n",
      "Sample 566: Predicted label: 2\n",
      "Sample 567: Predicted label: 2\n",
      "Sample 568: Predicted label: 2\n",
      "Sample 569: Predicted label: 2\n",
      "Sample 570: Predicted label: 2\n",
      "Sample 571: Predicted label: 2\n",
      "Sample 572: Predicted label: 2\n",
      "Sample 573: Predicted label: 2\n",
      "Sample 574: Predicted label: 1\n",
      "Sample 575: Predicted label: 2\n",
      "Sample 576: Predicted label: 2\n",
      "Sample 577: Predicted label: 2\n",
      "Sample 578: Predicted label: 2\n",
      "Sample 579: Predicted label: 2\n",
      "Sample 580: Predicted label: 2\n",
      "Sample 581: Predicted label: 2\n",
      "Sample 582: Predicted label: 2\n",
      "Sample 583: Predicted label: 2\n",
      "Sample 584: Predicted label: 2\n",
      "Sample 585: Predicted label: 2\n",
      "Sample 586: Predicted label: 2\n",
      "Sample 587: Predicted label: 2\n",
      "Sample 588: Predicted label: 2\n",
      "Sample 589: Predicted label: 2\n",
      "Sample 590: Predicted label: 2\n",
      "Sample 591: Predicted label: 2\n",
      "Sample 592: Predicted label: 2\n",
      "Sample 593: Predicted label: 2\n",
      "Sample 594: Predicted label: 2\n",
      "Sample 595: Predicted label: 2\n",
      "Sample 596: Predicted label: 2\n",
      "Sample 597: Predicted label: 2\n",
      "Sample 598: Predicted label: 2\n",
      "Sample 599: Predicted label: 2\n",
      "Sample 600: Predicted label: 2\n",
      "Sample 601: Predicted label: 2\n",
      "Sample 602: Predicted label: 2\n",
      "Sample 603: Predicted label: 2\n",
      "Sample 604: Predicted label: 2\n",
      "Sample 605: Predicted label: 2\n",
      "Sample 606: Predicted label: 2\n",
      "Sample 607: Predicted label: 1\n",
      "Sample 608: Predicted label: 2\n",
      "Sample 609: Predicted label: 2\n",
      "Sample 610: Predicted label: 2\n",
      "Sample 611: Predicted label: 2\n",
      "Sample 612: Predicted label: 1\n",
      "Sample 613: Predicted label: 1\n",
      "Sample 614: Predicted label: 2\n",
      "Sample 615: Predicted label: 2\n",
      "Sample 616: Predicted label: 2\n",
      "Sample 617: Predicted label: 2\n",
      "Sample 618: Predicted label: 2\n",
      "Sample 619: Predicted label: 2\n",
      "Sample 620: Predicted label: 2\n",
      "Sample 621: Predicted label: 2\n",
      "Sample 622: Predicted label: 2\n",
      "Sample 623: Predicted label: 2\n",
      "Sample 624: Predicted label: 2\n",
      "Sample 625: Predicted label: 2\n",
      "Sample 626: Predicted label: 2\n",
      "Sample 627: Predicted label: 2\n",
      "Sample 628: Predicted label: 2\n",
      "Sample 629: Predicted label: 2\n",
      "Sample 630: Predicted label: 2\n",
      "Sample 631: Predicted label: 2\n",
      "Sample 632: Predicted label: 2\n",
      "Sample 633: Predicted label: 2\n",
      "Sample 634: Predicted label: 2\n",
      "Sample 635: Predicted label: 2\n",
      "Sample 636: Predicted label: 2\n",
      "Sample 637: Predicted label: 2\n",
      "Sample 638: Predicted label: 2\n",
      "Sample 639: Predicted label: 2\n",
      "Sample 640: Predicted label: 2\n",
      "Sample 641: Predicted label: 2\n",
      "Sample 642: Predicted label: 2\n",
      "Sample 643: Predicted label: 2\n",
      "Sample 644: Predicted label: 2\n",
      "Sample 645: Predicted label: 2\n",
      "Sample 646: Predicted label: 2\n",
      "Sample 647: Predicted label: 2\n",
      "Sample 648: Predicted label: 2\n",
      "Sample 649: Predicted label: 2\n",
      "Sample 650: Predicted label: 2\n",
      "Sample 651: Predicted label: 2\n",
      "Sample 652: Predicted label: 2\n",
      "Sample 653: Predicted label: 2\n",
      "Sample 654: Predicted label: 2\n",
      "Sample 655: Predicted label: 2\n",
      "Sample 656: Predicted label: 2\n",
      "Sample 657: Predicted label: 2\n",
      "Sample 658: Predicted label: 2\n",
      "Sample 659: Predicted label: 2\n",
      "Sample 660: Predicted label: 2\n",
      "Sample 661: Predicted label: 2\n",
      "Sample 662: Predicted label: 2\n",
      "Sample 663: Predicted label: 2\n",
      "Sample 664: Predicted label: 2\n",
      "Sample 665: Predicted label: 2\n",
      "Sample 666: Predicted label: 2\n",
      "Sample 667: Predicted label: 2\n",
      "Sample 668: Predicted label: 2\n",
      "Sample 669: Predicted label: 2\n",
      "Sample 670: Predicted label: 2\n",
      "Sample 671: Predicted label: 2\n",
      "Sample 672: Predicted label: 2\n",
      "Sample 673: Predicted label: 2\n",
      "Sample 674: Predicted label: 2\n",
      "Sample 675: Predicted label: 2\n",
      "Sample 676: Predicted label: 2\n",
      "Sample 677: Predicted label: 2\n",
      "Sample 678: Predicted label: 2\n",
      "Sample 679: Predicted label: 2\n",
      "Sample 680: Predicted label: 2\n",
      "Sample 681: Predicted label: 2\n",
      "Sample 682: Predicted label: 2\n",
      "Sample 683: Predicted label: 2\n",
      "Sample 684: Predicted label: 2\n",
      "Sample 685: Predicted label: 2\n",
      "Sample 686: Predicted label: 2\n",
      "Sample 687: Predicted label: 2\n",
      "Sample 688: Predicted label: 2\n",
      "Sample 689: Predicted label: 2\n",
      "Sample 690: Predicted label: 2\n",
      "Sample 691: Predicted label: 2\n",
      "Sample 692: Predicted label: 2\n",
      "Sample 693: Predicted label: 2\n",
      "Sample 694: Predicted label: 2\n",
      "Sample 695: Predicted label: 2\n",
      "Sample 696: Predicted label: 2\n",
      "Sample 697: Predicted label: 2\n",
      "Sample 698: Predicted label: 2\n",
      "Sample 699: Predicted label: 2\n",
      "Sample 700: Predicted label: 2\n",
      "Sample 701: Predicted label: 2\n",
      "Sample 702: Predicted label: 2\n",
      "Sample 703: Predicted label: 2\n",
      "Sample 704: Predicted label: 2\n",
      "Sample 705: Predicted label: 1\n",
      "Sample 706: Predicted label: 2\n",
      "Sample 707: Predicted label: 2\n",
      "Sample 708: Predicted label: 2\n",
      "Sample 709: Predicted label: 2\n",
      "Sample 710: Predicted label: 2\n",
      "Sample 711: Predicted label: 2\n",
      "Sample 712: Predicted label: 2\n",
      "Sample 713: Predicted label: 2\n",
      "Sample 714: Predicted label: 1\n",
      "Sample 715: Predicted label: 2\n",
      "Sample 716: Predicted label: 2\n",
      "Sample 717: Predicted label: 2\n",
      "Sample 718: Predicted label: 2\n",
      "Sample 719: Predicted label: 2\n",
      "Sample 720: Predicted label: 2\n",
      "Sample 721: Predicted label: 2\n",
      "Sample 722: Predicted label: 2\n",
      "Sample 723: Predicted label: 2\n",
      "Sample 724: Predicted label: 1\n",
      "Sample 725: Predicted label: 2\n",
      "Sample 726: Predicted label: 2\n",
      "Sample 727: Predicted label: 2\n",
      "Sample 728: Predicted label: 2\n",
      "Sample 729: Predicted label: 2\n",
      "Sample 730: Predicted label: 2\n",
      "Sample 731: Predicted label: 2\n",
      "Sample 732: Predicted label: 2\n",
      "Sample 733: Predicted label: 2\n",
      "Sample 734: Predicted label: 1\n",
      "Sample 735: Predicted label: 2\n",
      "Sample 736: Predicted label: 2\n",
      "Sample 737: Predicted label: 2\n",
      "Sample 738: Predicted label: 2\n",
      "Sample 739: Predicted label: 2\n",
      "Sample 740: Predicted label: 2\n",
      "Sample 741: Predicted label: 2\n",
      "Sample 742: Predicted label: 2\n",
      "Sample 743: Predicted label: 2\n",
      "Sample 744: Predicted label: 2\n",
      "Sample 745: Predicted label: 2\n",
      "Sample 746: Predicted label: 2\n",
      "Sample 747: Predicted label: 2\n",
      "Sample 748: Predicted label: 2\n",
      "Sample 749: Predicted label: 2\n",
      "Sample 750: Predicted label: 2\n",
      "Sample 751: Predicted label: 2\n",
      "Sample 752: Predicted label: 2\n",
      "Sample 753: Predicted label: 2\n",
      "Sample 754: Predicted label: 0\n",
      "Sample 755: Predicted label: 2\n",
      "Sample 756: Predicted label: 2\n",
      "Sample 757: Predicted label: 1\n",
      "Sample 758: Predicted label: 1\n",
      "Sample 759: Predicted label: 0\n",
      "Sample 760: Predicted label: 2\n",
      "Sample 761: Predicted label: 2\n",
      "Sample 762: Predicted label: 0\n",
      "Sample 763: Predicted label: 2\n",
      "Sample 764: Predicted label: 1\n",
      "Sample 765: Predicted label: 0\n",
      "Sample 766: Predicted label: 2\n",
      "Sample 767: Predicted label: 0\n",
      "Sample 768: Predicted label: 2\n",
      "Sample 769: Predicted label: 0\n",
      "Sample 770: Predicted label: 1\n",
      "Sample 771: Predicted label: 1\n",
      "Sample 772: Predicted label: 1\n",
      "Sample 773: Predicted label: 2\n",
      "Sample 774: Predicted label: 2\n",
      "Sample 775: Predicted label: 2\n",
      "Sample 776: Predicted label: 2\n",
      "Sample 777: Predicted label: 1\n",
      "Sample 778: Predicted label: 2\n",
      "Sample 779: Predicted label: 0\n",
      "Sample 780: Predicted label: 1\n",
      "Sample 781: Predicted label: 1\n",
      "Sample 782: Predicted label: 2\n",
      "Sample 783: Predicted label: 2\n",
      "Sample 784: Predicted label: 2\n",
      "Sample 785: Predicted label: 2\n",
      "Sample 786: Predicted label: 0\n",
      "Sample 787: Predicted label: 2\n",
      "Sample 788: Predicted label: 0\n",
      "Sample 789: Predicted label: 2\n",
      "Sample 790: Predicted label: 2\n",
      "Sample 791: Predicted label: 0\n",
      "Sample 792: Predicted label: 2\n",
      "Sample 793: Predicted label: 2\n",
      "Sample 794: Predicted label: 2\n",
      "Sample 795: Predicted label: 1\n",
      "Sample 796: Predicted label: 2\n",
      "Sample 797: Predicted label: 2\n",
      "Sample 798: Predicted label: 2\n",
      "Sample 799: Predicted label: 0\n",
      "Sample 800: Predicted label: 2\n",
      "Sample 801: Predicted label: 2\n",
      "Sample 802: Predicted label: 2\n",
      "Sample 803: Predicted label: 2\n",
      "Sample 804: Predicted label: 2\n",
      "Sample 805: Predicted label: 0\n",
      "Sample 806: Predicted label: 2\n",
      "Sample 807: Predicted label: 1\n",
      "Sample 808: Predicted label: 2\n",
      "Sample 809: Predicted label: 0\n",
      "Sample 810: Predicted label: 0\n",
      "Sample 811: Predicted label: 2\n",
      "Sample 812: Predicted label: 0\n",
      "Sample 813: Predicted label: 1\n",
      "Sample 814: Predicted label: 2\n",
      "Sample 815: Predicted label: 2\n",
      "Sample 816: Predicted label: 2\n",
      "Sample 817: Predicted label: 2\n",
      "Sample 818: Predicted label: 2\n",
      "Sample 819: Predicted label: 2\n",
      "Sample 820: Predicted label: 2\n",
      "Sample 821: Predicted label: 2\n",
      "Sample 822: Predicted label: 2\n",
      "Sample 823: Predicted label: 2\n",
      "Sample 824: Predicted label: 2\n",
      "Sample 825: Predicted label: 2\n",
      "Sample 826: Predicted label: 2\n",
      "Sample 827: Predicted label: 2\n",
      "Sample 828: Predicted label: 2\n",
      "Sample 829: Predicted label: 2\n",
      "Sample 830: Predicted label: 2\n",
      "Sample 831: Predicted label: 2\n",
      "Sample 832: Predicted label: 2\n",
      "Sample 833: Predicted label: 2\n",
      "Sample 834: Predicted label: 2\n",
      "Sample 835: Predicted label: 1\n",
      "Sample 836: Predicted label: 1\n",
      "Sample 837: Predicted label: 2\n",
      "Sample 838: Predicted label: 1\n",
      "Sample 839: Predicted label: 2\n",
      "Sample 840: Predicted label: 2\n",
      "Sample 841: Predicted label: 2\n",
      "Sample 842: Predicted label: 2\n",
      "Sample 843: Predicted label: 1\n",
      "Sample 844: Predicted label: 1\n",
      "Sample 845: Predicted label: 0\n",
      "Sample 846: Predicted label: 1\n",
      "Sample 847: Predicted label: 2\n",
      "Sample 848: Predicted label: 0\n",
      "Sample 849: Predicted label: 0\n",
      "Sample 850: Predicted label: 1\n",
      "Sample 851: Predicted label: 1\n",
      "Sample 852: Predicted label: 1\n",
      "Sample 853: Predicted label: 1\n",
      "Sample 854: Predicted label: 0\n",
      "Sample 855: Predicted label: 0\n",
      "Sample 856: Predicted label: 1\n",
      "Sample 857: Predicted label: 2\n",
      "Sample 858: Predicted label: 1\n",
      "Sample 859: Predicted label: 0\n",
      "Sample 860: Predicted label: 1\n",
      "Sample 861: Predicted label: 2\n",
      "Sample 862: Predicted label: 2\n",
      "Sample 863: Predicted label: 1\n",
      "Sample 864: Predicted label: 1\n",
      "Sample 865: Predicted label: 1\n",
      "Sample 866: Predicted label: 1\n",
      "Sample 867: Predicted label: 0\n",
      "Sample 868: Predicted label: 1\n",
      "Sample 869: Predicted label: 2\n",
      "Sample 870: Predicted label: 2\n",
      "Sample 871: Predicted label: 2\n",
      "Sample 872: Predicted label: 2\n",
      "Sample 873: Predicted label: 1\n",
      "Sample 874: Predicted label: 2\n",
      "Sample 875: Predicted label: 2\n",
      "Sample 876: Predicted label: 1\n",
      "Sample 877: Predicted label: 2\n",
      "Sample 878: Predicted label: 1\n",
      "Sample 879: Predicted label: 1\n",
      "Sample 880: Predicted label: 1\n",
      "Sample 881: Predicted label: 1\n",
      "Sample 882: Predicted label: 1\n",
      "Sample 883: Predicted label: 1\n",
      "Sample 884: Predicted label: 1\n",
      "Sample 885: Predicted label: 2\n",
      "Sample 886: Predicted label: 1\n",
      "Sample 887: Predicted label: 1\n",
      "Sample 888: Predicted label: 1\n",
      "Sample 889: Predicted label: 1\n",
      "Sample 890: Predicted label: 2\n",
      "Sample 891: Predicted label: 1\n",
      "Sample 892: Predicted label: 1\n",
      "Sample 893: Predicted label: 1\n",
      "Sample 894: Predicted label: 1\n",
      "Sample 895: Predicted label: 1\n",
      "Sample 896: Predicted label: 1\n",
      "Sample 897: Predicted label: 1\n",
      "Sample 898: Predicted label: 1\n",
      "Sample 899: Predicted label: 1\n",
      "Sample 900: Predicted label: 1\n",
      "Sample 901: Predicted label: 1\n",
      "Sample 902: Predicted label: 0\n",
      "Sample 903: Predicted label: 2\n",
      "Sample 904: Predicted label: 1\n",
      "Sample 905: Predicted label: 2\n",
      "Sample 906: Predicted label: 1\n",
      "Sample 907: Predicted label: 2\n",
      "Sample 908: Predicted label: 1\n",
      "Sample 909: Predicted label: 1\n",
      "Sample 910: Predicted label: 1\n",
      "Sample 911: Predicted label: 0\n",
      "Sample 912: Predicted label: 1\n",
      "Sample 913: Predicted label: 1\n",
      "Sample 914: Predicted label: 1\n",
      "Sample 915: Predicted label: 2\n",
      "Sample 916: Predicted label: 2\n",
      "Sample 917: Predicted label: 2\n",
      "Sample 918: Predicted label: 1\n",
      "Sample 919: Predicted label: 1\n",
      "Sample 920: Predicted label: 1\n",
      "Sample 921: Predicted label: 1\n",
      "Sample 922: Predicted label: 1\n",
      "Sample 923: Predicted label: 1\n",
      "Sample 924: Predicted label: 1\n",
      "Sample 925: Predicted label: 1\n",
      "Sample 926: Predicted label: 1\n",
      "Sample 927: Predicted label: 0\n",
      "Sample 928: Predicted label: 2\n",
      "Sample 929: Predicted label: 2\n",
      "Sample 930: Predicted label: 2\n",
      "Sample 931: Predicted label: 1\n",
      "Sample 932: Predicted label: 1\n",
      "Sample 933: Predicted label: 1\n",
      "Sample 934: Predicted label: 2\n",
      "Sample 935: Predicted label: 2\n",
      "Sample 936: Predicted label: 1\n",
      "Sample 937: Predicted label: 2\n",
      "Sample 938: Predicted label: 0\n",
      "Sample 939: Predicted label: 2\n",
      "Sample 940: Predicted label: 2\n",
      "Sample 941: Predicted label: 2\n",
      "Sample 942: Predicted label: 0\n",
      "Sample 943: Predicted label: 1\n",
      "Sample 944: Predicted label: 1\n",
      "Sample 945: Predicted label: 0\n",
      "Sample 946: Predicted label: 2\n",
      "Sample 947: Predicted label: 1\n",
      "Sample 948: Predicted label: 1\n",
      "Sample 949: Predicted label: 2\n",
      "Sample 950: Predicted label: 2\n",
      "Sample 951: Predicted label: 2\n",
      "Sample 952: Predicted label: 0\n",
      "Sample 953: Predicted label: 0\n",
      "Sample 954: Predicted label: 2\n",
      "Sample 955: Predicted label: 1\n",
      "Sample 956: Predicted label: 2\n",
      "Sample 957: Predicted label: 2\n",
      "Sample 958: Predicted label: 2\n",
      "Sample 959: Predicted label: 2\n",
      "Sample 960: Predicted label: 2\n",
      "Sample 961: Predicted label: 2\n",
      "Sample 962: Predicted label: 2\n",
      "Sample 963: Predicted label: 1\n",
      "Sample 964: Predicted label: 2\n",
      "Sample 965: Predicted label: 2\n",
      "Sample 966: Predicted label: 2\n",
      "Sample 967: Predicted label: 2\n",
      "Sample 968: Predicted label: 2\n",
      "Sample 969: Predicted label: 2\n",
      "Sample 970: Predicted label: 2\n",
      "Sample 971: Predicted label: 2\n",
      "Sample 972: Predicted label: 2\n",
      "Sample 973: Predicted label: 1\n",
      "Sample 974: Predicted label: 0\n",
      "Sample 975: Predicted label: 2\n",
      "Sample 976: Predicted label: 0\n",
      "Sample 977: Predicted label: 2\n",
      "Sample 978: Predicted label: 2\n",
      "Sample 979: Predicted label: 2\n",
      "Sample 980: Predicted label: 2\n",
      "Sample 981: Predicted label: 2\n",
      "Sample 982: Predicted label: 1\n",
      "Sample 983: Predicted label: 2\n",
      "Sample 984: Predicted label: 1\n",
      "Sample 985: Predicted label: 2\n",
      "Sample 986: Predicted label: 2\n",
      "Sample 987: Predicted label: 2\n",
      "Sample 988: Predicted label: 2\n",
      "Sample 989: Predicted label: 2\n",
      "Sample 990: Predicted label: 2\n",
      "Sample 991: Predicted label: 2\n",
      "Sample 992: Predicted label: 2\n",
      "Sample 993: Predicted label: 2\n",
      "Sample 994: Predicted label: 2\n",
      "Sample 995: Predicted label: 2\n",
      "Sample 996: Predicted label: 2\n",
      "Sample 997: Predicted label: 2\n",
      "Sample 998: Predicted label: 2\n",
      "Sample 999: Predicted label: 2\n",
      "Sample 1000: Predicted label: 2\n",
      "Sample 1001: Predicted label: 2\n",
      "Sample 1002: Predicted label: 2\n",
      "Sample 1003: Predicted label: 2\n",
      "Sample 1004: Predicted label: 0\n",
      "Sample 1005: Predicted label: 2\n",
      "Sample 1006: Predicted label: 2\n",
      "Sample 1007: Predicted label: 2\n",
      "Sample 1008: Predicted label: 2\n",
      "Sample 1009: Predicted label: 2\n",
      "Sample 1010: Predicted label: 2\n",
      "Sample 1011: Predicted label: 2\n",
      "Sample 1012: Predicted label: 1\n",
      "Sample 1013: Predicted label: 2\n",
      "Sample 1014: Predicted label: 2\n",
      "Sample 1015: Predicted label: 2\n",
      "Sample 1016: Predicted label: 2\n",
      "Sample 1017: Predicted label: 2\n",
      "Sample 1018: Predicted label: 2\n",
      "Sample 1019: Predicted label: 2\n",
      "Sample 1020: Predicted label: 2\n",
      "Sample 1021: Predicted label: 2\n",
      "Sample 1022: Predicted label: 2\n",
      "Sample 1023: Predicted label: 2\n",
      "Sample 1024: Predicted label: 2\n",
      "Sample 1025: Predicted label: 2\n",
      "Sample 1026: Predicted label: 2\n",
      "Sample 1027: Predicted label: 1\n",
      "Sample 1028: Predicted label: 1\n",
      "Sample 1029: Predicted label: 2\n",
      "Sample 1030: Predicted label: 2\n",
      "Sample 1031: Predicted label: 2\n",
      "Sample 1032: Predicted label: 0\n",
      "Sample 1033: Predicted label: 2\n",
      "Sample 1034: Predicted label: 2\n",
      "Sample 1035: Predicted label: 2\n",
      "Sample 1036: Predicted label: 2\n",
      "Sample 1037: Predicted label: 2\n",
      "Sample 1038: Predicted label: 2\n",
      "Sample 1039: Predicted label: 2\n",
      "Sample 1040: Predicted label: 2\n",
      "Sample 1041: Predicted label: 2\n",
      "Sample 1042: Predicted label: 2\n",
      "Sample 1043: Predicted label: 2\n",
      "Sample 1044: Predicted label: 2\n",
      "Sample 1045: Predicted label: 2\n",
      "Sample 1046: Predicted label: 2\n",
      "Sample 1047: Predicted label: 2\n",
      "Sample 1048: Predicted label: 2\n",
      "Sample 1049: Predicted label: 2\n",
      "Sample 1050: Predicted label: 2\n",
      "Sample 1051: Predicted label: 0\n",
      "Sample 1052: Predicted label: 2\n",
      "Sample 1053: Predicted label: 2\n",
      "Sample 1054: Predicted label: 2\n",
      "Sample 1055: Predicted label: 2\n",
      "Sample 1056: Predicted label: 2\n",
      "Sample 1057: Predicted label: 2\n",
      "Sample 1058: Predicted label: 2\n",
      "Sample 1059: Predicted label: 2\n",
      "Sample 1060: Predicted label: 2\n",
      "Sample 1061: Predicted label: 2\n",
      "Sample 1062: Predicted label: 2\n",
      "Sample 1063: Predicted label: 2\n",
      "Sample 1064: Predicted label: 2\n",
      "Sample 1065: Predicted label: 2\n",
      "Sample 1066: Predicted label: 2\n",
      "Sample 1067: Predicted label: 2\n",
      "Sample 1068: Predicted label: 2\n",
      "Sample 1069: Predicted label: 2\n",
      "Sample 1070: Predicted label: 2\n",
      "Sample 1071: Predicted label: 2\n",
      "Sample 1072: Predicted label: 2\n",
      "Sample 1073: Predicted label: 2\n",
      "Sample 1074: Predicted label: 2\n",
      "Sample 1075: Predicted label: 2\n",
      "Sample 1076: Predicted label: 2\n",
      "Sample 1077: Predicted label: 2\n",
      "Sample 1078: Predicted label: 2\n",
      "Sample 1079: Predicted label: 2\n",
      "Sample 1080: Predicted label: 2\n",
      "Sample 1081: Predicted label: 2\n",
      "Sample 1082: Predicted label: 2\n",
      "Sample 1083: Predicted label: 2\n",
      "Sample 1084: Predicted label: 2\n",
      "Sample 1085: Predicted label: 2\n",
      "Sample 1086: Predicted label: 2\n",
      "Sample 1087: Predicted label: 2\n",
      "Sample 1088: Predicted label: 2\n",
      "Sample 1089: Predicted label: 2\n",
      "Sample 1090: Predicted label: 2\n",
      "Sample 1091: Predicted label: 0\n",
      "Sample 1092: Predicted label: 2\n",
      "Sample 1093: Predicted label: 2\n",
      "Sample 1094: Predicted label: 2\n",
      "Sample 1095: Predicted label: 2\n",
      "Sample 1096: Predicted label: 2\n",
      "Sample 1097: Predicted label: 2\n",
      "Sample 1098: Predicted label: 2\n",
      "Sample 1099: Predicted label: 2\n",
      "Sample 1100: Predicted label: 2\n",
      "Sample 1101: Predicted label: 2\n",
      "Sample 1102: Predicted label: 2\n",
      "Sample 1103: Predicted label: 2\n",
      "Sample 1104: Predicted label: 2\n",
      "Sample 1105: Predicted label: 2\n",
      "Sample 1106: Predicted label: 2\n",
      "Sample 1107: Predicted label: 2\n",
      "Sample 1108: Predicted label: 2\n",
      "Sample 1109: Predicted label: 2\n",
      "Sample 1110: Predicted label: 2\n",
      "Sample 1111: Predicted label: 2\n",
      "Sample 1112: Predicted label: 2\n",
      "Sample 1113: Predicted label: 2\n",
      "Sample 1114: Predicted label: 2\n",
      "Sample 1115: Predicted label: 2\n",
      "Sample 1116: Predicted label: 2\n",
      "Sample 1117: Predicted label: 2\n",
      "Sample 1118: Predicted label: 0\n",
      "Sample 1119: Predicted label: 2\n",
      "Sample 1120: Predicted label: 2\n",
      "Sample 1121: Predicted label: 2\n",
      "Sample 1122: Predicted label: 2\n",
      "Sample 1123: Predicted label: 2\n",
      "Sample 1124: Predicted label: 2\n",
      "Sample 1125: Predicted label: 2\n",
      "Sample 1126: Predicted label: 2\n",
      "Sample 1127: Predicted label: 2\n",
      "Sample 1128: Predicted label: 2\n",
      "Sample 1129: Predicted label: 2\n",
      "Sample 1130: Predicted label: 2\n",
      "Sample 1131: Predicted label: 2\n",
      "Sample 1132: Predicted label: 2\n",
      "Sample 1133: Predicted label: 2\n",
      "Sample 1134: Predicted label: 2\n",
      "Sample 1135: Predicted label: 2\n",
      "Sample 1136: Predicted label: 2\n",
      "Sample 1137: Predicted label: 2\n",
      "Sample 1138: Predicted label: 2\n",
      "Sample 1139: Predicted label: 0\n",
      "Sample 1140: Predicted label: 2\n",
      "Sample 1141: Predicted label: 2\n",
      "Sample 1142: Predicted label: 2\n",
      "Sample 1143: Predicted label: 2\n",
      "Sample 1144: Predicted label: 2\n",
      "Sample 1145: Predicted label: 2\n",
      "Sample 1146: Predicted label: 2\n",
      "Sample 1147: Predicted label: 2\n",
      "Sample 1148: Predicted label: 2\n",
      "Sample 1149: Predicted label: 2\n",
      "Sample 1150: Predicted label: 2\n",
      "Sample 1151: Predicted label: 2\n",
      "Sample 1152: Predicted label: 2\n",
      "Sample 1153: Predicted label: 2\n",
      "Sample 1154: Predicted label: 2\n",
      "Sample 1155: Predicted label: 2\n",
      "Sample 1156: Predicted label: 2\n",
      "Sample 1157: Predicted label: 2\n",
      "Sample 1158: Predicted label: 2\n",
      "Sample 1159: Predicted label: 2\n",
      "Sample 1160: Predicted label: 2\n",
      "Sample 1161: Predicted label: 2\n",
      "Sample 1162: Predicted label: 2\n",
      "Sample 1163: Predicted label: 2\n",
      "Sample 1164: Predicted label: 2\n",
      "Sample 1165: Predicted label: 2\n",
      "Sample 1166: Predicted label: 2\n",
      "Sample 1167: Predicted label: 2\n",
      "Sample 1168: Predicted label: 2\n",
      "Sample 1169: Predicted label: 2\n",
      "Sample 1170: Predicted label: 2\n",
      "Sample 1171: Predicted label: 2\n",
      "Sample 1172: Predicted label: 2\n",
      "Sample 1173: Predicted label: 2\n",
      "Sample 1174: Predicted label: 2\n",
      "Sample 1175: Predicted label: 2\n",
      "Sample 1176: Predicted label: 2\n",
      "Sample 1177: Predicted label: 2\n",
      "Sample 1178: Predicted label: 2\n",
      "Sample 1179: Predicted label: 2\n",
      "Sample 1180: Predicted label: 2\n",
      "Sample 1181: Predicted label: 2\n",
      "Sample 1182: Predicted label: 2\n",
      "Sample 1183: Predicted label: 2\n",
      "Sample 1184: Predicted label: 2\n",
      "Sample 1185: Predicted label: 2\n",
      "Sample 1186: Predicted label: 2\n",
      "Sample 1187: Predicted label: 2\n",
      "Sample 1188: Predicted label: 2\n",
      "Sample 1189: Predicted label: 2\n",
      "Sample 1190: Predicted label: 2\n",
      "Sample 1191: Predicted label: 2\n",
      "Sample 1192: Predicted label: 2\n",
      "Sample 1193: Predicted label: 2\n",
      "Sample 1194: Predicted label: 2\n",
      "Sample 1195: Predicted label: 2\n",
      "Sample 1196: Predicted label: 2\n",
      "Sample 1197: Predicted label: 2\n",
      "Sample 1198: Predicted label: 2\n",
      "Sample 1199: Predicted label: 2\n",
      "Sample 1200: Predicted label: 2\n",
      "Sample 1201: Predicted label: 2\n",
      "Sample 1202: Predicted label: 2\n",
      "Sample 1203: Predicted label: 1\n",
      "Sample 1204: Predicted label: 2\n",
      "Sample 1205: Predicted label: 2\n",
      "Sample 1206: Predicted label: 2\n",
      "Sample 1207: Predicted label: 2\n",
      "Sample 1208: Predicted label: 2\n",
      "Sample 1209: Predicted label: 2\n",
      "Sample 1210: Predicted label: 2\n",
      "Sample 1211: Predicted label: 2\n",
      "Sample 1212: Predicted label: 2\n",
      "Sample 1213: Predicted label: 2\n",
      "Sample 1214: Predicted label: 2\n",
      "Sample 1215: Predicted label: 2\n",
      "Sample 1216: Predicted label: 2\n",
      "Sample 1217: Predicted label: 2\n",
      "Sample 1218: Predicted label: 2\n",
      "Sample 1219: Predicted label: 2\n",
      "Sample 1220: Predicted label: 2\n",
      "Sample 1221: Predicted label: 2\n",
      "Sample 1222: Predicted label: 2\n",
      "Sample 1223: Predicted label: 2\n",
      "Sample 1224: Predicted label: 2\n",
      "Sample 1225: Predicted label: 2\n",
      "Sample 1226: Predicted label: 2\n",
      "Sample 1227: Predicted label: 2\n",
      "Sample 1228: Predicted label: 2\n",
      "Sample 1229: Predicted label: 2\n",
      "Sample 1230: Predicted label: 2\n",
      "Sample 1231: Predicted label: 2\n",
      "Sample 1232: Predicted label: 2\n",
      "Sample 1233: Predicted label: 2\n",
      "Sample 1234: Predicted label: 2\n",
      "Sample 1235: Predicted label: 2\n",
      "Sample 1236: Predicted label: 2\n",
      "Sample 1237: Predicted label: 2\n",
      "Sample 1238: Predicted label: 1\n",
      "Sample 1239: Predicted label: 2\n",
      "Sample 1240: Predicted label: 2\n",
      "Sample 1241: Predicted label: 2\n",
      "Sample 1242: Predicted label: 2\n",
      "Sample 1243: Predicted label: 2\n",
      "Sample 1244: Predicted label: 2\n",
      "Sample 1245: Predicted label: 2\n",
      "Sample 1246: Predicted label: 2\n",
      "Sample 1247: Predicted label: 2\n",
      "Sample 1248: Predicted label: 2\n",
      "Sample 1249: Predicted label: 2\n",
      "Sample 1250: Predicted label: 2\n",
      "Sample 1251: Predicted label: 2\n",
      "Sample 1252: Predicted label: 2\n",
      "Sample 1253: Predicted label: 2\n",
      "Sample 1254: Predicted label: 2\n",
      "Sample 1255: Predicted label: 2\n",
      "Sample 1256: Predicted label: 0\n",
      "Sample 1257: Predicted label: 2\n",
      "Sample 1258: Predicted label: 2\n",
      "Sample 1259: Predicted label: 2\n",
      "Sample 1260: Predicted label: 2\n",
      "Sample 1261: Predicted label: 2\n",
      "Sample 1262: Predicted label: 2\n",
      "Sample 1263: Predicted label: 2\n",
      "Sample 1264: Predicted label: 2\n",
      "Sample 1265: Predicted label: 2\n",
      "Sample 1266: Predicted label: 2\n",
      "Sample 1267: Predicted label: 2\n",
      "Sample 1268: Predicted label: 2\n",
      "Sample 1269: Predicted label: 2\n",
      "Sample 1270: Predicted label: 2\n",
      "Sample 1271: Predicted label: 2\n",
      "Sample 1272: Predicted label: 2\n",
      "Sample 1273: Predicted label: 2\n",
      "Sample 1274: Predicted label: 2\n",
      "Sample 1275: Predicted label: 2\n",
      "Sample 1276: Predicted label: 1\n",
      "Sample 1277: Predicted label: 1\n",
      "Sample 1278: Predicted label: 2\n",
      "Sample 1279: Predicted label: 2\n",
      "Sample 1280: Predicted label: 2\n",
      "Sample 1281: Predicted label: 2\n",
      "Sample 1282: Predicted label: 2\n",
      "Sample 1283: Predicted label: 2\n",
      "Sample 1284: Predicted label: 2\n",
      "Sample 1285: Predicted label: 2\n",
      "Sample 1286: Predicted label: 2\n",
      "Sample 1287: Predicted label: 2\n",
      "Sample 1288: Predicted label: 2\n",
      "Sample 1289: Predicted label: 2\n",
      "Sample 1290: Predicted label: 2\n",
      "Sample 1291: Predicted label: 2\n",
      "Sample 1292: Predicted label: 1\n",
      "Sample 1293: Predicted label: 1\n",
      "Sample 1294: Predicted label: 2\n",
      "Sample 1295: Predicted label: 2\n",
      "Sample 1296: Predicted label: 2\n",
      "Sample 1297: Predicted label: 2\n",
      "Sample 1298: Predicted label: 2\n",
      "Sample 1299: Predicted label: 2\n",
      "Sample 1300: Predicted label: 2\n",
      "Sample 1301: Predicted label: 2\n",
      "Sample 1302: Predicted label: 2\n",
      "Sample 1303: Predicted label: 2\n",
      "Sample 1304: Predicted label: 2\n",
      "Sample 1305: Predicted label: 2\n",
      "Sample 1306: Predicted label: 2\n",
      "Sample 1307: Predicted label: 2\n",
      "Sample 1308: Predicted label: 2\n",
      "Sample 1309: Predicted label: 2\n",
      "Sample 1310: Predicted label: 2\n",
      "Sample 1311: Predicted label: 2\n",
      "Sample 1312: Predicted label: 2\n",
      "Sample 1313: Predicted label: 2\n",
      "Sample 1314: Predicted label: 2\n",
      "Sample 1315: Predicted label: 2\n",
      "Sample 1316: Predicted label: 2\n",
      "Sample 1317: Predicted label: 2\n",
      "Sample 1318: Predicted label: 2\n",
      "Sample 1319: Predicted label: 2\n",
      "Sample 1320: Predicted label: 2\n",
      "Sample 1321: Predicted label: 2\n",
      "Sample 1322: Predicted label: 2\n",
      "Sample 1323: Predicted label: 2\n",
      "Sample 1324: Predicted label: 2\n",
      "Sample 1325: Predicted label: 2\n",
      "Sample 1326: Predicted label: 2\n",
      "Sample 1327: Predicted label: 2\n",
      "Sample 1328: Predicted label: 2\n",
      "Sample 1329: Predicted label: 2\n",
      "Sample 1330: Predicted label: 0\n",
      "Sample 1331: Predicted label: 2\n",
      "Sample 1332: Predicted label: 2\n",
      "Sample 1333: Predicted label: 2\n",
      "Sample 1334: Predicted label: 2\n",
      "Sample 1335: Predicted label: 2\n",
      "Sample 1336: Predicted label: 2\n",
      "Sample 1337: Predicted label: 2\n",
      "Sample 1338: Predicted label: 2\n",
      "Sample 1339: Predicted label: 2\n",
      "Sample 1340: Predicted label: 2\n",
      "Sample 1341: Predicted label: 2\n",
      "Sample 1342: Predicted label: 2\n",
      "Sample 1343: Predicted label: 2\n",
      "Sample 1344: Predicted label: 2\n",
      "Sample 1345: Predicted label: 2\n",
      "Sample 1346: Predicted label: 2\n",
      "Sample 1347: Predicted label: 2\n",
      "Sample 1348: Predicted label: 2\n",
      "Sample 1349: Predicted label: 2\n",
      "Sample 1350: Predicted label: 2\n",
      "Sample 1351: Predicted label: 2\n",
      "Sample 1352: Predicted label: 2\n",
      "Sample 1353: Predicted label: 2\n",
      "Sample 1354: Predicted label: 2\n",
      "Sample 1355: Predicted label: 2\n",
      "Sample 1356: Predicted label: 2\n",
      "Sample 1357: Predicted label: 2\n",
      "Sample 1358: Predicted label: 2\n",
      "Sample 1359: Predicted label: 2\n",
      "Sample 1360: Predicted label: 2\n",
      "Sample 1361: Predicted label: 2\n",
      "Sample 1362: Predicted label: 2\n",
      "Sample 1363: Predicted label: 2\n",
      "Sample 1364: Predicted label: 2\n",
      "Sample 1365: Predicted label: 2\n",
      "Sample 1366: Predicted label: 2\n",
      "Sample 1367: Predicted label: 2\n",
      "Sample 1368: Predicted label: 2\n",
      "Sample 1369: Predicted label: 2\n",
      "Sample 1370: Predicted label: 2\n",
      "Sample 1371: Predicted label: 2\n",
      "Sample 1372: Predicted label: 2\n",
      "Sample 1373: Predicted label: 2\n",
      "Sample 1374: Predicted label: 2\n",
      "Sample 1375: Predicted label: 2\n",
      "Sample 1376: Predicted label: 2\n",
      "Sample 1377: Predicted label: 2\n",
      "Sample 1378: Predicted label: 2\n",
      "Sample 1379: Predicted label: 2\n",
      "Sample 1380: Predicted label: 2\n",
      "Sample 1381: Predicted label: 2\n",
      "Sample 1382: Predicted label: 2\n",
      "Sample 1383: Predicted label: 2\n",
      "Sample 1384: Predicted label: 1\n",
      "Sample 1385: Predicted label: 2\n",
      "Sample 1386: Predicted label: 2\n",
      "Sample 1387: Predicted label: 2\n",
      "Sample 1388: Predicted label: 2\n",
      "Sample 1389: Predicted label: 2\n",
      "Sample 1390: Predicted label: 2\n",
      "Sample 1391: Predicted label: 2\n",
      "Sample 1392: Predicted label: 2\n",
      "Sample 1393: Predicted label: 2\n",
      "Sample 1394: Predicted label: 2\n",
      "Sample 1395: Predicted label: 2\n",
      "Sample 1396: Predicted label: 2\n",
      "Sample 1397: Predicted label: 2\n",
      "Sample 1398: Predicted label: 2\n",
      "Sample 1399: Predicted label: 2\n",
      "Sample 1400: Predicted label: 2\n",
      "Sample 1401: Predicted label: 2\n",
      "Sample 1402: Predicted label: 2\n",
      "Sample 1403: Predicted label: 2\n",
      "Sample 1404: Predicted label: 1\n",
      "Sample 1405: Predicted label: 2\n",
      "Sample 1406: Predicted label: 0\n",
      "Sample 1407: Predicted label: 2\n",
      "Sample 1408: Predicted label: 2\n",
      "Sample 1409: Predicted label: 2\n",
      "Sample 1410: Predicted label: 1\n",
      "Sample 1411: Predicted label: 2\n",
      "Sample 1412: Predicted label: 1\n",
      "Sample 1413: Predicted label: 2\n",
      "Sample 1414: Predicted label: 2\n",
      "Sample 1415: Predicted label: 2\n",
      "Sample 1416: Predicted label: 2\n",
      "Sample 1417: Predicted label: 1\n",
      "Sample 1418: Predicted label: 1\n",
      "Sample 1419: Predicted label: 2\n",
      "Sample 1420: Predicted label: 1\n",
      "Sample 1421: Predicted label: 1\n",
      "Sample 1422: Predicted label: 1\n",
      "Sample 1423: Predicted label: 1\n",
      "Sample 1424: Predicted label: 2\n",
      "Sample 1425: Predicted label: 1\n",
      "Sample 1426: Predicted label: 2\n",
      "Sample 1427: Predicted label: 2\n",
      "Sample 1428: Predicted label: 2\n",
      "Sample 1429: Predicted label: 2\n",
      "Sample 1430: Predicted label: 2\n",
      "Sample 1431: Predicted label: 2\n",
      "Sample 1432: Predicted label: 1\n",
      "Sample 1433: Predicted label: 2\n",
      "Sample 1434: Predicted label: 2\n",
      "Sample 1435: Predicted label: 2\n",
      "Sample 1436: Predicted label: 2\n",
      "Sample 1437: Predicted label: 2\n",
      "Sample 1438: Predicted label: 2\n",
      "Sample 1439: Predicted label: 1\n",
      "Sample 1440: Predicted label: 2\n",
      "Sample 1441: Predicted label: 2\n",
      "Sample 1442: Predicted label: 2\n",
      "Sample 1443: Predicted label: 2\n",
      "Sample 1444: Predicted label: 2\n",
      "Sample 1445: Predicted label: 1\n",
      "Sample 1446: Predicted label: 2\n",
      "Sample 1447: Predicted label: 2\n",
      "Sample 1448: Predicted label: 2\n",
      "Sample 1449: Predicted label: 2\n",
      "Sample 1450: Predicted label: 2\n",
      "Sample 1451: Predicted label: 2\n",
      "Sample 1452: Predicted label: 0\n",
      "Sample 1453: Predicted label: 2\n",
      "Sample 1454: Predicted label: 2\n",
      "Sample 1455: Predicted label: 2\n",
      "Sample 1456: Predicted label: 2\n",
      "Sample 1457: Predicted label: 0\n",
      "Sample 1458: Predicted label: 2\n",
      "Sample 1459: Predicted label: 2\n",
      "Sample 1460: Predicted label: 2\n",
      "Sample 1461: Predicted label: 2\n",
      "Sample 1462: Predicted label: 2\n",
      "Sample 1463: Predicted label: 2\n",
      "Sample 1464: Predicted label: 2\n",
      "Sample 1465: Predicted label: 2\n",
      "Sample 1466: Predicted label: 2\n",
      "Sample 1467: Predicted label: 2\n",
      "Sample 1468: Predicted label: 2\n",
      "Sample 1469: Predicted label: 2\n",
      "Sample 1470: Predicted label: 2\n",
      "Sample 1471: Predicted label: 2\n",
      "Sample 1472: Predicted label: 2\n",
      "Sample 1473: Predicted label: 2\n",
      "Sample 1474: Predicted label: 2\n",
      "Sample 1475: Predicted label: 2\n",
      "Sample 1476: Predicted label: 2\n",
      "Sample 1477: Predicted label: 1\n",
      "Sample 1478: Predicted label: 2\n",
      "Sample 1479: Predicted label: 2\n",
      "Sample 1480: Predicted label: 2\n",
      "Sample 1481: Predicted label: 2\n",
      "Sample 1482: Predicted label: 2\n",
      "Sample 1483: Predicted label: 2\n",
      "Sample 1484: Predicted label: 2\n",
      "Sample 1485: Predicted label: 2\n",
      "Sample 1486: Predicted label: 2\n",
      "Sample 1487: Predicted label: 2\n",
      "Sample 1488: Predicted label: 2\n",
      "Sample 1489: Predicted label: 2\n",
      "Sample 1490: Predicted label: 2\n",
      "Sample 1491: Predicted label: 2\n",
      "Sample 1492: Predicted label: 2\n",
      "Sample 1493: Predicted label: 1\n",
      "Sample 1494: Predicted label: 1\n",
      "Sample 1495: Predicted label: 1\n",
      "Sample 1496: Predicted label: 2\n",
      "Sample 1497: Predicted label: 2\n",
      "Sample 1498: Predicted label: 2\n",
      "Sample 1499: Predicted label: 2\n",
      "Sample 1500: Predicted label: 2\n",
      "Sample 1501: Predicted label: 2\n",
      "Sample 1502: Predicted label: 2\n",
      "Sample 1503: Predicted label: 1\n",
      "Sample 1504: Predicted label: 2\n",
      "Sample 1505: Predicted label: 2\n",
      "Sample 1506: Predicted label: 2\n",
      "Sample 1507: Predicted label: 2\n",
      "Sample 1508: Predicted label: 2\n",
      "Sample 1509: Predicted label: 2\n",
      "Sample 1510: Predicted label: 2\n",
      "Sample 1511: Predicted label: 2\n",
      "Sample 1512: Predicted label: 2\n",
      "Sample 1513: Predicted label: 2\n",
      "Sample 1514: Predicted label: 2\n",
      "Sample 1515: Predicted label: 2\n",
      "Sample 1516: Predicted label: 2\n",
      "Sample 1517: Predicted label: 2\n",
      "Sample 1518: Predicted label: 2\n",
      "Sample 1519: Predicted label: 1\n",
      "Sample 1520: Predicted label: 1\n",
      "Sample 1521: Predicted label: 2\n",
      "Sample 1522: Predicted label: 2\n",
      "Sample 1523: Predicted label: 2\n",
      "Sample 1524: Predicted label: 2\n",
      "Sample 1525: Predicted label: 2\n",
      "Sample 1526: Predicted label: 1\n",
      "Sample 1527: Predicted label: 2\n",
      "Sample 1528: Predicted label: 2\n",
      "Sample 1529: Predicted label: 2\n",
      "Sample 1530: Predicted label: 2\n",
      "Sample 1531: Predicted label: 2\n",
      "Sample 1532: Predicted label: 2\n",
      "Sample 1533: Predicted label: 2\n",
      "Sample 1534: Predicted label: 1\n",
      "Sample 1535: Predicted label: 2\n",
      "Sample 1536: Predicted label: 2\n",
      "Sample 1537: Predicted label: 2\n",
      "Sample 1538: Predicted label: 1\n",
      "Sample 1539: Predicted label: 2\n",
      "Sample 1540: Predicted label: 2\n",
      "Sample 1541: Predicted label: 2\n",
      "Sample 1542: Predicted label: 2\n",
      "Sample 1543: Predicted label: 2\n",
      "Sample 1544: Predicted label: 2\n",
      "Sample 1545: Predicted label: 2\n",
      "Sample 1546: Predicted label: 2\n",
      "Sample 1547: Predicted label: 2\n",
      "Sample 1548: Predicted label: 2\n",
      "Sample 1549: Predicted label: 2\n",
      "Sample 1550: Predicted label: 2\n",
      "Sample 1551: Predicted label: 2\n",
      "Sample 1552: Predicted label: 1\n",
      "Sample 1553: Predicted label: 0\n",
      "Sample 1554: Predicted label: 2\n",
      "Sample 1555: Predicted label: 0\n",
      "Sample 1556: Predicted label: 2\n",
      "Sample 1557: Predicted label: 0\n",
      "Sample 1558: Predicted label: 2\n",
      "Sample 1559: Predicted label: 2\n",
      "Sample 1560: Predicted label: 0\n",
      "Sample 1561: Predicted label: 0\n",
      "Sample 1562: Predicted label: 2\n",
      "Sample 1563: Predicted label: 2\n",
      "Sample 1564: Predicted label: 0\n",
      "Sample 1565: Predicted label: 2\n",
      "Sample 1566: Predicted label: 2\n",
      "Sample 1567: Predicted label: 0\n",
      "Sample 1568: Predicted label: 2\n",
      "Sample 1569: Predicted label: 1\n",
      "Sample 1570: Predicted label: 0\n",
      "Sample 1571: Predicted label: 2\n",
      "Sample 1572: Predicted label: 0\n",
      "Sample 1573: Predicted label: 2\n",
      "Sample 1574: Predicted label: 2\n",
      "Sample 1575: Predicted label: 2\n",
      "Sample 1576: Predicted label: 2\n",
      "Sample 1577: Predicted label: 2\n",
      "Sample 1578: Predicted label: 0\n",
      "Sample 1579: Predicted label: 2\n",
      "Sample 1580: Predicted label: 2\n",
      "Sample 1581: Predicted label: 2\n",
      "Sample 1582: Predicted label: 2\n",
      "Sample 1583: Predicted label: 1\n",
      "Sample 1584: Predicted label: 0\n",
      "Sample 1585: Predicted label: 2\n",
      "Sample 1586: Predicted label: 2\n",
      "Sample 1587: Predicted label: 2\n",
      "Sample 1588: Predicted label: 2\n",
      "Sample 1589: Predicted label: 2\n",
      "Sample 1590: Predicted label: 0\n",
      "Sample 1591: Predicted label: 1\n",
      "Sample 1592: Predicted label: 1\n",
      "Sample 1593: Predicted label: 2\n",
      "Sample 1594: Predicted label: 1\n",
      "Sample 1595: Predicted label: 1\n",
      "Sample 1596: Predicted label: 1\n",
      "Sample 1597: Predicted label: 2\n",
      "Sample 1598: Predicted label: 1\n",
      "Sample 1599: Predicted label: 2\n",
      "Sample 1600: Predicted label: 2\n",
      "Sample 1601: Predicted label: 2\n",
      "Sample 1602: Predicted label: 2\n",
      "Sample 1603: Predicted label: 2\n",
      "Sample 1604: Predicted label: 2\n",
      "Sample 1605: Predicted label: 2\n",
      "Sample 1606: Predicted label: 2\n",
      "Sample 1607: Predicted label: 2\n",
      "Sample 1608: Predicted label: 1\n",
      "Sample 1609: Predicted label: 1\n",
      "Sample 1610: Predicted label: 2\n",
      "Sample 1611: Predicted label: 2\n",
      "Sample 1612: Predicted label: 2\n",
      "Sample 1613: Predicted label: 2\n",
      "Sample 1614: Predicted label: 2\n",
      "Sample 1615: Predicted label: 1\n",
      "Sample 1616: Predicted label: 2\n",
      "Sample 1617: Predicted label: 1\n",
      "Sample 1618: Predicted label: 1\n",
      "Sample 1619: Predicted label: 2\n",
      "Sample 1620: Predicted label: 1\n",
      "Sample 1621: Predicted label: 1\n",
      "Sample 1622: Predicted label: 1\n",
      "Sample 1623: Predicted label: 2\n",
      "Sample 1624: Predicted label: 2\n",
      "Sample 1625: Predicted label: 1\n",
      "Sample 1626: Predicted label: 1\n",
      "Sample 1627: Predicted label: 2\n",
      "Sample 1628: Predicted label: 2\n",
      "Sample 1629: Predicted label: 2\n",
      "Sample 1630: Predicted label: 1\n",
      "Sample 1631: Predicted label: 1\n",
      "Sample 1632: Predicted label: 2\n",
      "Sample 1633: Predicted label: 2\n",
      "Sample 1634: Predicted label: 2\n",
      "Sample 1635: Predicted label: 2\n",
      "Sample 1636: Predicted label: 0\n",
      "Sample 1637: Predicted label: 2\n",
      "Sample 1638: Predicted label: 2\n",
      "Sample 1639: Predicted label: 2\n",
      "Sample 1640: Predicted label: 2\n",
      "Sample 1641: Predicted label: 2\n",
      "Sample 1642: Predicted label: 2\n",
      "Sample 1643: Predicted label: 2\n",
      "Sample 1644: Predicted label: 0\n",
      "Sample 1645: Predicted label: 2\n",
      "Sample 1646: Predicted label: 2\n",
      "Sample 1647: Predicted label: 0\n",
      "Sample 1648: Predicted label: 2\n",
      "Sample 1649: Predicted label: 2\n",
      "Sample 1650: Predicted label: 2\n",
      "Sample 1651: Predicted label: 2\n",
      "Sample 1652: Predicted label: 2\n",
      "Sample 1653: Predicted label: 1\n",
      "Sample 1654: Predicted label: 2\n",
      "Sample 1655: Predicted label: 2\n",
      "Sample 1656: Predicted label: 2\n",
      "Sample 1657: Predicted label: 2\n",
      "Sample 1658: Predicted label: 2\n",
      "Sample 1659: Predicted label: 1\n",
      "Sample 1660: Predicted label: 2\n",
      "Sample 1661: Predicted label: 2\n",
      "Sample 1662: Predicted label: 0\n",
      "Sample 1663: Predicted label: 2\n",
      "Sample 1664: Predicted label: 0\n",
      "Sample 1665: Predicted label: 2\n",
      "Sample 1666: Predicted label: 2\n",
      "Sample 1667: Predicted label: 2\n",
      "Sample 1668: Predicted label: 2\n",
      "Sample 1669: Predicted label: 2\n",
      "Sample 1670: Predicted label: 2\n",
      "Sample 1671: Predicted label: 1\n",
      "Sample 1672: Predicted label: 0\n",
      "Sample 1673: Predicted label: 1\n",
      "Sample 1674: Predicted label: 2\n",
      "Sample 1675: Predicted label: 1\n",
      "Sample 1676: Predicted label: 0\n",
      "Sample 1677: Predicted label: 0\n",
      "Sample 1678: Predicted label: 0\n",
      "Sample 1679: Predicted label: 0\n",
      "Sample 1680: Predicted label: 0\n",
      "Sample 1681: Predicted label: 1\n",
      "Sample 1682: Predicted label: 0\n",
      "Sample 1683: Predicted label: 1\n",
      "Sample 1684: Predicted label: 2\n",
      "Sample 1685: Predicted label: 0\n",
      "Sample 1686: Predicted label: 1\n",
      "Sample 1687: Predicted label: 0\n",
      "Sample 1688: Predicted label: 2\n",
      "Sample 1689: Predicted label: 0\n",
      "Sample 1690: Predicted label: 0\n",
      "Sample 1691: Predicted label: 0\n",
      "Sample 1692: Predicted label: 2\n",
      "Sample 1693: Predicted label: 2\n",
      "Sample 1694: Predicted label: 1\n",
      "Sample 1695: Predicted label: 1\n",
      "Sample 1696: Predicted label: 1\n",
      "Sample 1697: Predicted label: 2\n",
      "Sample 1698: Predicted label: 0\n",
      "Sample 1699: Predicted label: 1\n",
      "Sample 1700: Predicted label: 2\n",
      "Sample 1701: Predicted label: 2\n",
      "Sample 1702: Predicted label: 1\n",
      "Sample 1703: Predicted label: 2\n",
      "Sample 1704: Predicted label: 1\n",
      "Sample 1705: Predicted label: 2\n",
      "Sample 1706: Predicted label: 2\n",
      "Sample 1707: Predicted label: 0\n",
      "Sample 1708: Predicted label: 2\n",
      "Sample 1709: Predicted label: 0\n",
      "Sample 1710: Predicted label: 2\n",
      "Sample 1711: Predicted label: 1\n",
      "Sample 1712: Predicted label: 2\n",
      "Sample 1713: Predicted label: 2\n",
      "Sample 1714: Predicted label: 1\n",
      "Sample 1715: Predicted label: 1\n",
      "Sample 1716: Predicted label: 2\n",
      "Sample 1717: Predicted label: 1\n",
      "Sample 1718: Predicted label: 1\n",
      "Sample 1719: Predicted label: 1\n",
      "Sample 1720: Predicted label: 1\n",
      "Sample 1721: Predicted label: 2\n",
      "Sample 1722: Predicted label: 0\n",
      "Sample 1723: Predicted label: 1\n",
      "Sample 1724: Predicted label: 1\n",
      "Sample 1725: Predicted label: 1\n",
      "Sample 1726: Predicted label: 1\n",
      "Sample 1727: Predicted label: 0\n",
      "Sample 1728: Predicted label: 1\n",
      "Sample 1729: Predicted label: 1\n",
      "Sample 1730: Predicted label: 1\n",
      "Sample 1731: Predicted label: 2\n",
      "Sample 1732: Predicted label: 1\n",
      "Sample 1733: Predicted label: 2\n",
      "Sample 1734: Predicted label: 1\n",
      "Sample 1735: Predicted label: 1\n",
      "Sample 1736: Predicted label: 0\n",
      "Sample 1737: Predicted label: 2\n",
      "Sample 1738: Predicted label: 2\n",
      "Sample 1739: Predicted label: 1\n",
      "Sample 1740: Predicted label: 1\n",
      "Sample 1741: Predicted label: 2\n",
      "Sample 1742: Predicted label: 1\n",
      "Sample 1743: Predicted label: 1\n",
      "Sample 1744: Predicted label: 2\n",
      "Sample 1745: Predicted label: 2\n",
      "Sample 1746: Predicted label: 2\n",
      "Sample 1747: Predicted label: 2\n",
      "Sample 1748: Predicted label: 2\n",
      "Sample 1749: Predicted label: 0\n",
      "Sample 1750: Predicted label: 2\n",
      "Sample 1751: Predicted label: 2\n",
      "Sample 1752: Predicted label: 1\n",
      "Sample 1753: Predicted label: 2\n",
      "Sample 1754: Predicted label: 2\n",
      "Sample 1755: Predicted label: 2\n",
      "Sample 1756: Predicted label: 2\n",
      "Sample 1757: Predicted label: 2\n",
      "Sample 1758: Predicted label: 2\n",
      "Sample 1759: Predicted label: 2\n",
      "Sample 1760: Predicted label: 2\n",
      "Sample 1761: Predicted label: 2\n",
      "Sample 1762: Predicted label: 2\n",
      "Sample 1763: Predicted label: 2\n",
      "Sample 1764: Predicted label: 2\n",
      "Sample 1765: Predicted label: 2\n",
      "Sample 1766: Predicted label: 1\n",
      "Sample 1767: Predicted label: 1\n",
      "Sample 1768: Predicted label: 1\n",
      "Sample 1769: Predicted label: 0\n",
      "Sample 1770: Predicted label: 2\n",
      "Sample 1771: Predicted label: 2\n",
      "Sample 1772: Predicted label: 2\n",
      "Sample 1773: Predicted label: 1\n",
      "Sample 1774: Predicted label: 2\n",
      "Sample 1775: Predicted label: 2\n",
      "Sample 1776: Predicted label: 2\n",
      "Sample 1777: Predicted label: 2\n",
      "Sample 1778: Predicted label: 2\n",
      "Sample 1779: Predicted label: 2\n",
      "Sample 1780: Predicted label: 2\n",
      "Sample 1781: Predicted label: 2\n",
      "Sample 1782: Predicted label: 2\n",
      "Sample 1783: Predicted label: 2\n",
      "Sample 1784: Predicted label: 2\n",
      "Sample 1785: Predicted label: 2\n",
      "Sample 1786: Predicted label: 2\n",
      "Sample 1787: Predicted label: 2\n",
      "Sample 1788: Predicted label: 2\n",
      "Sample 1789: Predicted label: 2\n",
      "Sample 1790: Predicted label: 2\n",
      "Sample 1791: Predicted label: 2\n",
      "Sample 1792: Predicted label: 2\n",
      "Sample 1793: Predicted label: 2\n",
      "Sample 1794: Predicted label: 2\n",
      "Sample 1795: Predicted label: 0\n",
      "Sample 1796: Predicted label: 2\n",
      "Sample 1797: Predicted label: 2\n",
      "Sample 1798: Predicted label: 2\n",
      "Sample 1799: Predicted label: 2\n",
      "Sample 1800: Predicted label: 2\n",
      "Sample 1801: Predicted label: 2\n",
      "Sample 1802: Predicted label: 2\n",
      "Sample 1803: Predicted label: 2\n",
      "Sample 1804: Predicted label: 2\n",
      "Sample 1805: Predicted label: 2\n",
      "Sample 1806: Predicted label: 2\n",
      "Sample 1807: Predicted label: 2\n",
      "Sample 1808: Predicted label: 2\n",
      "Sample 1809: Predicted label: 2\n",
      "Sample 1810: Predicted label: 2\n",
      "Sample 1811: Predicted label: 2\n",
      "Sample 1812: Predicted label: 2\n",
      "Sample 1813: Predicted label: 2\n",
      "Sample 1814: Predicted label: 2\n",
      "Sample 1815: Predicted label: 1\n",
      "Sample 1816: Predicted label: 2\n",
      "Sample 1817: Predicted label: 2\n",
      "Sample 1818: Predicted label: 2\n",
      "Sample 1819: Predicted label: 2\n",
      "Sample 1820: Predicted label: 2\n",
      "Sample 1821: Predicted label: 2\n",
      "Sample 1822: Predicted label: 1\n",
      "Sample 1823: Predicted label: 2\n",
      "Sample 1824: Predicted label: 2\n",
      "Sample 1825: Predicted label: 2\n",
      "Sample 1826: Predicted label: 2\n",
      "Sample 1827: Predicted label: 2\n",
      "Sample 1828: Predicted label: 2\n",
      "Sample 1829: Predicted label: 2\n",
      "Sample 1830: Predicted label: 2\n",
      "Sample 1831: Predicted label: 2\n",
      "Sample 1832: Predicted label: 2\n",
      "Sample 1833: Predicted label: 2\n",
      "Sample 1834: Predicted label: 2\n",
      "Sample 1835: Predicted label: 2\n",
      "Sample 1836: Predicted label: 2\n",
      "Sample 1837: Predicted label: 2\n",
      "Sample 1838: Predicted label: 2\n",
      "Sample 1839: Predicted label: 2\n",
      "Sample 1840: Predicted label: 2\n",
      "Sample 1841: Predicted label: 2\n",
      "Sample 1842: Predicted label: 2\n",
      "Sample 1843: Predicted label: 2\n",
      "Sample 1844: Predicted label: 2\n",
      "Sample 1845: Predicted label: 2\n",
      "Sample 1846: Predicted label: 2\n",
      "Sample 1847: Predicted label: 2\n",
      "Sample 1848: Predicted label: 2\n",
      "Sample 1849: Predicted label: 2\n",
      "Sample 1850: Predicted label: 2\n",
      "Sample 1851: Predicted label: 2\n",
      "Sample 1852: Predicted label: 2\n",
      "Sample 1853: Predicted label: 2\n",
      "Sample 1854: Predicted label: 2\n",
      "Sample 1855: Predicted label: 2\n",
      "Sample 1856: Predicted label: 2\n",
      "Sample 1857: Predicted label: 2\n",
      "Sample 1858: Predicted label: 2\n",
      "Sample 1859: Predicted label: 2\n",
      "Sample 1860: Predicted label: 2\n",
      "Sample 1861: Predicted label: 2\n",
      "Sample 1862: Predicted label: 2\n",
      "Sample 1863: Predicted label: 2\n",
      "Sample 1864: Predicted label: 2\n",
      "Sample 1865: Predicted label: 2\n",
      "Sample 1866: Predicted label: 2\n",
      "Sample 1867: Predicted label: 2\n",
      "Sample 1868: Predicted label: 2\n",
      "Sample 1869: Predicted label: 2\n",
      "Sample 1870: Predicted label: 2\n",
      "Sample 1871: Predicted label: 2\n",
      "Sample 1872: Predicted label: 2\n",
      "Sample 1873: Predicted label: 2\n",
      "Sample 1874: Predicted label: 2\n",
      "Sample 1875: Predicted label: 2\n",
      "Sample 1876: Predicted label: 2\n",
      "Sample 1877: Predicted label: 2\n",
      "Sample 1878: Predicted label: 2\n",
      "Sample 1879: Predicted label: 2\n",
      "Sample 1880: Predicted label: 2\n",
      "Sample 1881: Predicted label: 2\n",
      "Sample 1882: Predicted label: 2\n",
      "Sample 1883: Predicted label: 2\n",
      "Sample 1884: Predicted label: 2\n",
      "Sample 1885: Predicted label: 2\n",
      "Sample 1886: Predicted label: 2\n",
      "Sample 1887: Predicted label: 2\n",
      "Sample 1888: Predicted label: 2\n",
      "Sample 1889: Predicted label: 2\n",
      "Sample 1890: Predicted label: 2\n",
      "Sample 1891: Predicted label: 2\n",
      "Sample 1892: Predicted label: 2\n",
      "Sample 1893: Predicted label: 2\n",
      "Sample 1894: Predicted label: 2\n",
      "Sample 1895: Predicted label: 2\n",
      "Sample 1896: Predicted label: 2\n",
      "Sample 1897: Predicted label: 2\n",
      "Sample 1898: Predicted label: 2\n",
      "Sample 1899: Predicted label: 2\n",
      "Sample 1900: Predicted label: 2\n",
      "Sample 1901: Predicted label: 2\n",
      "Sample 1902: Predicted label: 2\n",
      "Sample 1903: Predicted label: 2\n",
      "Sample 1904: Predicted label: 2\n",
      "Sample 1905: Predicted label: 2\n",
      "Sample 1906: Predicted label: 2\n",
      "Sample 1907: Predicted label: 2\n",
      "Sample 1908: Predicted label: 2\n",
      "Sample 1909: Predicted label: 2\n",
      "Sample 1910: Predicted label: 2\n",
      "Sample 1911: Predicted label: 2\n",
      "Sample 1912: Predicted label: 1\n",
      "Sample 1913: Predicted label: 1\n",
      "Sample 1914: Predicted label: 0\n",
      "Sample 1915: Predicted label: 2\n",
      "Sample 1916: Predicted label: 2\n",
      "Sample 1917: Predicted label: 2\n",
      "Sample 1918: Predicted label: 2\n",
      "Sample 1919: Predicted label: 2\n",
      "Sample 1920: Predicted label: 0\n",
      "Sample 1921: Predicted label: 2\n",
      "Sample 1922: Predicted label: 2\n",
      "Sample 1923: Predicted label: 2\n",
      "Sample 1924: Predicted label: 2\n",
      "Sample 1925: Predicted label: 1\n",
      "Sample 1926: Predicted label: 0\n",
      "Sample 1927: Predicted label: 1\n",
      "Sample 1928: Predicted label: 1\n",
      "Sample 1929: Predicted label: 2\n",
      "Sample 1930: Predicted label: 2\n",
      "Sample 1931: Predicted label: 2\n",
      "Sample 1932: Predicted label: 2\n",
      "Sample 1933: Predicted label: 1\n",
      "Sample 1934: Predicted label: 1\n",
      "Sample 1935: Predicted label: 2\n",
      "Sample 1936: Predicted label: 1\n",
      "Sample 1937: Predicted label: 1\n",
      "Sample 1938: Predicted label: 0\n",
      "Sample 1939: Predicted label: 1\n",
      "Sample 1940: Predicted label: 1\n",
      "Sample 1941: Predicted label: 2\n",
      "Sample 1942: Predicted label: 1\n",
      "Sample 1943: Predicted label: 1\n",
      "Sample 1944: Predicted label: 1\n",
      "Sample 1945: Predicted label: 1\n",
      "Sample 1946: Predicted label: 2\n",
      "Sample 1947: Predicted label: 1\n",
      "Sample 1948: Predicted label: 1\n",
      "Sample 1949: Predicted label: 1\n",
      "Sample 1950: Predicted label: 2\n",
      "Sample 1951: Predicted label: 1\n",
      "Sample 1952: Predicted label: 2\n",
      "Sample 1953: Predicted label: 1\n",
      "Sample 1954: Predicted label: 2\n",
      "Sample 1955: Predicted label: 1\n",
      "Sample 1956: Predicted label: 2\n",
      "Sample 1957: Predicted label: 1\n",
      "Sample 1958: Predicted label: 2\n",
      "Sample 1959: Predicted label: 1\n",
      "Sample 1960: Predicted label: 2\n",
      "Sample 1961: Predicted label: 2\n",
      "Sample 1962: Predicted label: 1\n",
      "Sample 1963: Predicted label: 2\n",
      "Sample 1964: Predicted label: 2\n",
      "Sample 1965: Predicted label: 1\n",
      "Sample 1966: Predicted label: 1\n",
      "Sample 1967: Predicted label: 2\n",
      "Sample 1968: Predicted label: 0\n",
      "Sample 1969: Predicted label: 1\n",
      "Sample 1970: Predicted label: 2\n",
      "Sample 1971: Predicted label: 2\n",
      "Sample 1972: Predicted label: 1\n",
      "Sample 1973: Predicted label: 1\n",
      "Sample 1974: Predicted label: 2\n",
      "Sample 1975: Predicted label: 1\n",
      "Sample 1976: Predicted label: 2\n",
      "Sample 1977: Predicted label: 1\n",
      "Sample 1978: Predicted label: 1\n",
      "Sample 1979: Predicted label: 1\n",
      "Sample 1980: Predicted label: 2\n",
      "Sample 1981: Predicted label: 2\n",
      "Sample 1982: Predicted label: 0\n",
      "Sample 1983: Predicted label: 1\n",
      "Sample 1984: Predicted label: 2\n",
      "Sample 1985: Predicted label: 1\n",
      "Sample 1986: Predicted label: 2\n",
      "Sample 1987: Predicted label: 1\n",
      "Sample 1988: Predicted label: 1\n",
      "Sample 1989: Predicted label: 1\n",
      "Sample 1990: Predicted label: 1\n",
      "Sample 1991: Predicted label: 1\n",
      "Sample 1992: Predicted label: 1\n",
      "Sample 1993: Predicted label: 1\n",
      "Sample 1994: Predicted label: 0\n",
      "Sample 1995: Predicted label: 1\n",
      "Sample 1996: Predicted label: 2\n",
      "Sample 1997: Predicted label: 0\n",
      "Sample 1998: Predicted label: 1\n",
      "Sample 1999: Predicted label: 1\n",
      "Sample 2000: Predicted label: 2\n",
      "Sample 2001: Predicted label: 1\n",
      "Sample 2002: Predicted label: 1\n",
      "Sample 2003: Predicted label: 1\n",
      "Sample 2004: Predicted label: 2\n",
      "Sample 2005: Predicted label: 2\n",
      "Sample 2006: Predicted label: 2\n",
      "Sample 2007: Predicted label: 1\n",
      "Sample 2008: Predicted label: 2\n",
      "Sample 2009: Predicted label: 1\n",
      "Sample 2010: Predicted label: 1\n",
      "Sample 2011: Predicted label: 1\n",
      "Sample 2012: Predicted label: 0\n",
      "Sample 2013: Predicted label: 1\n",
      "Sample 2014: Predicted label: 2\n",
      "Sample 2015: Predicted label: 1\n",
      "Sample 2016: Predicted label: 1\n",
      "Sample 2017: Predicted label: 2\n",
      "Sample 2018: Predicted label: 2\n",
      "Sample 2019: Predicted label: 2\n",
      "Sample 2020: Predicted label: 2\n",
      "Sample 2021: Predicted label: 0\n",
      "Sample 2022: Predicted label: 1\n",
      "Sample 2023: Predicted label: 1\n",
      "Sample 2024: Predicted label: 2\n",
      "Sample 2025: Predicted label: 2\n",
      "Sample 2026: Predicted label: 2\n",
      "Sample 2027: Predicted label: 2\n",
      "Sample 2028: Predicted label: 1\n",
      "Sample 2029: Predicted label: 2\n",
      "Sample 2030: Predicted label: 2\n",
      "Sample 2031: Predicted label: 2\n",
      "Sample 2032: Predicted label: 1\n",
      "Sample 2033: Predicted label: 2\n",
      "Sample 2034: Predicted label: 2\n",
      "Sample 2035: Predicted label: 1\n",
      "Sample 2036: Predicted label: 2\n",
      "Sample 2037: Predicted label: 1\n",
      "Sample 2038: Predicted label: 2\n",
      "Sample 2039: Predicted label: 2\n",
      "Sample 2040: Predicted label: 2\n",
      "Sample 2041: Predicted label: 2\n",
      "Sample 2042: Predicted label: 2\n",
      "Sample 2043: Predicted label: 2\n",
      "Sample 2044: Predicted label: 2\n",
      "Sample 2045: Predicted label: 2\n",
      "Sample 2046: Predicted label: 2\n",
      "Sample 2047: Predicted label: 2\n",
      "Sample 2048: Predicted label: 2\n",
      "Sample 2049: Predicted label: 2\n",
      "Sample 2050: Predicted label: 2\n",
      "Sample 2051: Predicted label: 1\n",
      "Sample 2052: Predicted label: 1\n",
      "Sample 2053: Predicted label: 2\n",
      "Sample 2054: Predicted label: 2\n",
      "Sample 2055: Predicted label: 1\n",
      "Sample 2056: Predicted label: 1\n",
      "Sample 2057: Predicted label: 2\n",
      "Sample 2058: Predicted label: 1\n",
      "Sample 2059: Predicted label: 2\n",
      "Sample 2060: Predicted label: 2\n",
      "Sample 2061: Predicted label: 1\n",
      "Sample 2062: Predicted label: 1\n",
      "Sample 2063: Predicted label: 1\n",
      "Sample 2064: Predicted label: 2\n",
      "Sample 2065: Predicted label: 1\n",
      "Sample 2066: Predicted label: 2\n",
      "Sample 2067: Predicted label: 2\n",
      "Sample 2068: Predicted label: 2\n",
      "Sample 2069: Predicted label: 1\n",
      "Sample 2070: Predicted label: 2\n",
      "Sample 2071: Predicted label: 1\n",
      "Sample 2072: Predicted label: 2\n",
      "Sample 2073: Predicted label: 2\n",
      "Sample 2074: Predicted label: 2\n",
      "Sample 2075: Predicted label: 2\n",
      "Sample 2076: Predicted label: 2\n",
      "Sample 2077: Predicted label: 2\n",
      "Sample 2078: Predicted label: 2\n",
      "Sample 2079: Predicted label: 2\n",
      "Sample 2080: Predicted label: 2\n",
      "Sample 2081: Predicted label: 1\n",
      "Sample 2082: Predicted label: 2\n",
      "Sample 2083: Predicted label: 2\n",
      "Sample 2084: Predicted label: 1\n",
      "Sample 2085: Predicted label: 2\n",
      "Sample 2086: Predicted label: 2\n",
      "Sample 2087: Predicted label: 2\n",
      "Sample 2088: Predicted label: 1\n",
      "Sample 2089: Predicted label: 2\n",
      "Sample 2090: Predicted label: 2\n",
      "Sample 2091: Predicted label: 1\n",
      "Sample 2092: Predicted label: 2\n",
      "Sample 2093: Predicted label: 2\n",
      "Sample 2094: Predicted label: 1\n",
      "Sample 2095: Predicted label: 2\n",
      "Sample 2096: Predicted label: 2\n",
      "Sample 2097: Predicted label: 2\n",
      "Sample 2098: Predicted label: 2\n",
      "Sample 2099: Predicted label: 2\n",
      "Sample 2100: Predicted label: 1\n",
      "Sample 2101: Predicted label: 1\n",
      "Sample 2102: Predicted label: 2\n",
      "Sample 2103: Predicted label: 2\n",
      "Sample 2104: Predicted label: 2\n",
      "Sample 2105: Predicted label: 2\n",
      "Sample 2106: Predicted label: 2\n",
      "Sample 2107: Predicted label: 2\n",
      "Sample 2108: Predicted label: 2\n",
      "Sample 2109: Predicted label: 2\n",
      "Sample 2110: Predicted label: 2\n",
      "Sample 2111: Predicted label: 2\n",
      "Sample 2112: Predicted label: 2\n",
      "Sample 2113: Predicted label: 2\n",
      "Sample 2114: Predicted label: 2\n",
      "Sample 2115: Predicted label: 1\n",
      "Sample 2116: Predicted label: 2\n",
      "Sample 2117: Predicted label: 2\n",
      "Sample 2118: Predicted label: 2\n",
      "Sample 2119: Predicted label: 2\n",
      "Sample 2120: Predicted label: 2\n",
      "Sample 2121: Predicted label: 2\n",
      "Sample 2122: Predicted label: 2\n",
      "Sample 2123: Predicted label: 2\n",
      "Sample 2124: Predicted label: 2\n",
      "Sample 2125: Predicted label: 2\n",
      "Sample 2126: Predicted label: 1\n",
      "Sample 2127: Predicted label: 2\n",
      "Sample 2128: Predicted label: 2\n",
      "Sample 2129: Predicted label: 1\n",
      "Sample 2130: Predicted label: 1\n",
      "Sample 2131: Predicted label: 2\n",
      "Sample 2132: Predicted label: 2\n",
      "Sample 2133: Predicted label: 2\n",
      "Sample 2134: Predicted label: 2\n",
      "Sample 2135: Predicted label: 2\n",
      "Sample 2136: Predicted label: 1\n",
      "Sample 2137: Predicted label: 0\n",
      "Sample 2138: Predicted label: 1\n",
      "Sample 2139: Predicted label: 1\n",
      "Sample 2140: Predicted label: 2\n",
      "Sample 2141: Predicted label: 2\n",
      "Sample 2142: Predicted label: 2\n",
      "Sample 2143: Predicted label: 1\n",
      "Sample 2144: Predicted label: 2\n",
      "Sample 2145: Predicted label: 2\n",
      "Sample 2146: Predicted label: 2\n",
      "Sample 2147: Predicted label: 2\n",
      "Sample 2148: Predicted label: 2\n",
      "Sample 2149: Predicted label: 2\n",
      "Sample 2150: Predicted label: 2\n",
      "Sample 2151: Predicted label: 2\n",
      "Sample 2152: Predicted label: 2\n",
      "Sample 2153: Predicted label: 2\n",
      "Sample 2154: Predicted label: 2\n",
      "Sample 2155: Predicted label: 2\n",
      "Sample 2156: Predicted label: 1\n",
      "Sample 2157: Predicted label: 1\n",
      "Sample 2158: Predicted label: 2\n",
      "Sample 2159: Predicted label: 2\n",
      "Sample 2160: Predicted label: 2\n",
      "Sample 2161: Predicted label: 2\n",
      "Sample 2162: Predicted label: 2\n",
      "Sample 2163: Predicted label: 2\n",
      "Sample 2164: Predicted label: 2\n",
      "Sample 2165: Predicted label: 2\n",
      "Sample 2166: Predicted label: 2\n",
      "Sample 2167: Predicted label: 2\n",
      "Sample 2168: Predicted label: 2\n",
      "Sample 2169: Predicted label: 2\n",
      "Sample 2170: Predicted label: 2\n",
      "Sample 2171: Predicted label: 2\n",
      "Sample 2172: Predicted label: 2\n",
      "Sample 2173: Predicted label: 2\n",
      "Sample 2174: Predicted label: 2\n",
      "Sample 2175: Predicted label: 2\n",
      "Sample 2176: Predicted label: 2\n",
      "Sample 2177: Predicted label: 2\n",
      "Sample 2178: Predicted label: 2\n",
      "Sample 2179: Predicted label: 2\n",
      "Sample 2180: Predicted label: 2\n",
      "Sample 2181: Predicted label: 2\n",
      "Sample 2182: Predicted label: 2\n",
      "Sample 2183: Predicted label: 2\n",
      "Sample 2184: Predicted label: 2\n",
      "Sample 2185: Predicted label: 2\n",
      "Sample 2186: Predicted label: 2\n",
      "Sample 2187: Predicted label: 2\n",
      "Sample 2188: Predicted label: 1\n",
      "Sample 2189: Predicted label: 1\n",
      "Sample 2190: Predicted label: 2\n",
      "Sample 2191: Predicted label: 2\n",
      "Sample 2192: Predicted label: 2\n",
      "Sample 2193: Predicted label: 1\n",
      "Sample 2194: Predicted label: 1\n",
      "Sample 2195: Predicted label: 2\n",
      "Sample 2196: Predicted label: 2\n",
      "Sample 2197: Predicted label: 2\n",
      "Sample 2198: Predicted label: 2\n",
      "Sample 2199: Predicted label: 2\n",
      "Sample 2200: Predicted label: 2\n",
      "Sample 2201: Predicted label: 2\n",
      "Sample 2202: Predicted label: 2\n",
      "Sample 2203: Predicted label: 2\n",
      "Sample 2204: Predicted label: 2\n",
      "Sample 2205: Predicted label: 2\n",
      "Sample 2206: Predicted label: 2\n",
      "Sample 2207: Predicted label: 2\n",
      "Sample 2208: Predicted label: 2\n",
      "Sample 2209: Predicted label: 2\n",
      "Sample 2210: Predicted label: 2\n",
      "Sample 2211: Predicted label: 2\n",
      "Sample 2212: Predicted label: 2\n",
      "Sample 2213: Predicted label: 2\n",
      "Sample 2214: Predicted label: 2\n",
      "Sample 2215: Predicted label: 1\n",
      "Sample 2216: Predicted label: 1\n",
      "Sample 2217: Predicted label: 1\n",
      "Sample 2218: Predicted label: 1\n",
      "Sample 2219: Predicted label: 2\n",
      "Sample 2220: Predicted label: 2\n",
      "Sample 2221: Predicted label: 2\n",
      "Sample 2222: Predicted label: 2\n",
      "Sample 2223: Predicted label: 2\n",
      "Sample 2224: Predicted label: 2\n",
      "Sample 2225: Predicted label: 1\n",
      "Sample 2226: Predicted label: 2\n",
      "Sample 2227: Predicted label: 2\n",
      "Sample 2228: Predicted label: 2\n",
      "Sample 2229: Predicted label: 2\n",
      "Sample 2230: Predicted label: 2\n",
      "Sample 2231: Predicted label: 2\n",
      "Sample 2232: Predicted label: 2\n",
      "Sample 2233: Predicted label: 2\n",
      "Sample 2234: Predicted label: 2\n",
      "Sample 2235: Predicted label: 0\n",
      "Sample 2236: Predicted label: 2\n",
      "Sample 2237: Predicted label: 2\n",
      "Sample 2238: Predicted label: 1\n",
      "Sample 2239: Predicted label: 1\n",
      "Sample 2240: Predicted label: 2\n",
      "Sample 2241: Predicted label: 1\n",
      "Sample 2242: Predicted label: 1\n",
      "Sample 2243: Predicted label: 2\n",
      "Sample 2244: Predicted label: 2\n",
      "Sample 2245: Predicted label: 0\n",
      "Sample 2246: Predicted label: 0\n",
      "Sample 2247: Predicted label: 2\n",
      "Sample 2248: Predicted label: 1\n",
      "Sample 2249: Predicted label: 1\n",
      "Sample 2250: Predicted label: 0\n",
      "Sample 2251: Predicted label: 1\n",
      "Sample 2252: Predicted label: 0\n",
      "Sample 2253: Predicted label: 1\n",
      "Sample 2254: Predicted label: 1\n",
      "Sample 2255: Predicted label: 2\n",
      "Sample 2256: Predicted label: 1\n",
      "Sample 2257: Predicted label: 0\n",
      "Sample 2258: Predicted label: 1\n",
      "Sample 2259: Predicted label: 0\n",
      "Sample 2260: Predicted label: 2\n",
      "Sample 2261: Predicted label: 2\n",
      "Sample 2262: Predicted label: 2\n",
      "Sample 2263: Predicted label: 0\n",
      "Sample 2264: Predicted label: 0\n",
      "Sample 2265: Predicted label: 0\n",
      "Sample 2266: Predicted label: 2\n",
      "Sample 2267: Predicted label: 0\n",
      "Sample 2268: Predicted label: 1\n",
      "Sample 2269: Predicted label: 0\n",
      "Sample 2270: Predicted label: 1\n",
      "Sample 2271: Predicted label: 0\n",
      "Sample 2272: Predicted label: 0\n",
      "Sample 2273: Predicted label: 1\n",
      "Sample 2274: Predicted label: 2\n",
      "Sample 2275: Predicted label: 2\n",
      "Sample 2276: Predicted label: 2\n",
      "Sample 2277: Predicted label: 0\n",
      "Sample 2278: Predicted label: 2\n",
      "Sample 2279: Predicted label: 1\n",
      "Sample 2280: Predicted label: 0\n",
      "Sample 2281: Predicted label: 1\n",
      "Sample 2282: Predicted label: 1\n",
      "Sample 2283: Predicted label: 1\n",
      "Sample 2284: Predicted label: 1\n",
      "Sample 2285: Predicted label: 1\n",
      "Sample 2286: Predicted label: 0\n",
      "Sample 2287: Predicted label: 0\n",
      "Sample 2288: Predicted label: 1\n",
      "Sample 2289: Predicted label: 0\n",
      "Sample 2290: Predicted label: 0\n",
      "Sample 2291: Predicted label: 0\n",
      "Sample 2292: Predicted label: 0\n",
      "Sample 2293: Predicted label: 2\n",
      "Sample 2294: Predicted label: 2\n",
      "Sample 2295: Predicted label: 2\n",
      "Sample 2296: Predicted label: 1\n",
      "Sample 2297: Predicted label: 1\n",
      "Sample 2298: Predicted label: 1\n",
      "Sample 2299: Predicted label: 1\n",
      "Sample 2300: Predicted label: 2\n",
      "Sample 2301: Predicted label: 2\n",
      "Sample 2302: Predicted label: 2\n",
      "Sample 2303: Predicted label: 2\n",
      "Sample 2304: Predicted label: 1\n",
      "Sample 2305: Predicted label: 0\n",
      "Sample 2306: Predicted label: 1\n",
      "Sample 2307: Predicted label: 1\n",
      "Sample 2308: Predicted label: 1\n",
      "Sample 2309: Predicted label: 2\n",
      "Sample 2310: Predicted label: 1\n",
      "Sample 2311: Predicted label: 2\n",
      "Sample 2312: Predicted label: 1\n",
      "Sample 2313: Predicted label: 1\n",
      "Sample 2314: Predicted label: 1\n",
      "Sample 2315: Predicted label: 2\n",
      "Sample 2316: Predicted label: 2\n",
      "Sample 2317: Predicted label: 2\n",
      "Sample 2318: Predicted label: 2\n",
      "Sample 2319: Predicted label: 2\n",
      "Sample 2320: Predicted label: 1\n",
      "Sample 2321: Predicted label: 2\n",
      "Sample 2322: Predicted label: 1\n",
      "Sample 2323: Predicted label: 1\n",
      "Sample 2324: Predicted label: 1\n",
      "Sample 2325: Predicted label: 1\n",
      "Sample 2326: Predicted label: 1\n",
      "Sample 2327: Predicted label: 2\n",
      "Sample 2328: Predicted label: 1\n",
      "Sample 2329: Predicted label: 1\n",
      "Sample 2330: Predicted label: 2\n",
      "Sample 2331: Predicted label: 2\n",
      "Sample 2332: Predicted label: 0\n",
      "Sample 2333: Predicted label: 1\n",
      "Sample 2334: Predicted label: 1\n",
      "Sample 2335: Predicted label: 1\n",
      "Sample 2336: Predicted label: 1\n",
      "Sample 2337: Predicted label: 1\n",
      "Sample 2338: Predicted label: 1\n",
      "Sample 2339: Predicted label: 1\n",
      "Sample 2340: Predicted label: 1\n",
      "Sample 2341: Predicted label: 2\n",
      "Sample 2342: Predicted label: 1\n",
      "Sample 2343: Predicted label: 1\n",
      "Sample 2344: Predicted label: 1\n",
      "Sample 2345: Predicted label: 1\n",
      "Sample 2346: Predicted label: 1\n",
      "Sample 2347: Predicted label: 1\n",
      "Sample 2348: Predicted label: 1\n",
      "Sample 2349: Predicted label: 1\n",
      "Sample 2350: Predicted label: 1\n",
      "Sample 2351: Predicted label: 2\n",
      "Sample 2352: Predicted label: 0\n",
      "Sample 2353: Predicted label: 0\n",
      "Sample 2354: Predicted label: 0\n",
      "Sample 2355: Predicted label: 1\n",
      "Sample 2356: Predicted label: 1\n",
      "Sample 2357: Predicted label: 1\n",
      "Sample 2358: Predicted label: 1\n",
      "Sample 2359: Predicted label: 1\n",
      "Sample 2360: Predicted label: 1\n",
      "Sample 2361: Predicted label: 1\n",
      "Sample 2362: Predicted label: 2\n",
      "Sample 2363: Predicted label: 0\n",
      "Sample 2364: Predicted label: 1\n",
      "Sample 2365: Predicted label: 0\n",
      "Sample 2366: Predicted label: 1\n",
      "Sample 2367: Predicted label: 1\n",
      "Sample 2368: Predicted label: 1\n",
      "Sample 2369: Predicted label: 1\n",
      "Sample 2370: Predicted label: 1\n",
      "Sample 2371: Predicted label: 2\n",
      "Sample 2372: Predicted label: 1\n",
      "Sample 2373: Predicted label: 1\n",
      "Sample 2374: Predicted label: 1\n",
      "Sample 2375: Predicted label: 2\n",
      "Sample 2376: Predicted label: 0\n",
      "Sample 2377: Predicted label: 2\n",
      "Sample 2378: Predicted label: 2\n",
      "Sample 2379: Predicted label: 1\n",
      "Sample 2380: Predicted label: 2\n",
      "Sample 2381: Predicted label: 2\n",
      "Sample 2382: Predicted label: 2\n",
      "Sample 2383: Predicted label: 2\n",
      "Sample 2384: Predicted label: 2\n",
      "Sample 2385: Predicted label: 2\n",
      "Sample 2386: Predicted label: 2\n",
      "Sample 2387: Predicted label: 2\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer_lora(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Run inference on the dataset\n",
    "predictions = []\n",
    "inference_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(tokenized_dataset)):\n",
    "        inputs = {key: val.unsqueeze(0).to(device) for key, val in tokenized_dataset[i].items()}\n",
    "        outputs = inference_model(**inputs)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "# Print predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Sample {i}: Predicted label: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b807e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt2-lora</th>\n",
       "      <td>0.548</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Balanced Accuracy  Precision  Recall     F1\n",
       "gpt2-lora              0.548       0.62   0.548  0.563"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " classification_scores('gpt2-lora', tokenized_dataset['label'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7159b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567208d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a6a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightweightFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
