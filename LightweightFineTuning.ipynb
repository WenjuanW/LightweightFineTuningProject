{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "This project is to classify emotions using a foundation model (GPT2). The purpose is to compare before and after light weight fine-tuning, how the model performances. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here are the choices for PEFT techique, foundation model used, evaluation approach and the dataset for fine-tuning:\n",
    "\n",
    "* PEFT technique: Lora techique\n",
    "* Model: GPT-2\n",
    "* Evaluation approach: Classification evaluation approaches such as accuracy, confusion matrix, auc, f1 score, precision-recall curve\n",
    "* Fine-tuning dataset: zeroshot/twitter-financial-news-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e0f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports modules\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8b18c",
   "metadata": {},
   "source": [
    "### Load the dataset dair-ai/emotion from datasets \n",
    "\n",
    "* Three splits in the dataset including train, validation and test\n",
    "* We have 6 class labels - what are they?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9543\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2388\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"zeroshot/twitter-financial-news-sentiment\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da832576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1442), (1, 1923), (2, 6178)]\n"
     ]
    }
   ],
   "source": [
    "# number of labels\n",
    "counts = Counter(dataset[\"train\"][\"label\"])\n",
    "sorted_counts = sorted(counts.items())\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=0, text=$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT\n",
      "label=0, text=$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3\n",
      "label=0, text=$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb\n",
      "label=0, text=$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N\n",
      "label=0, text=$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB\n",
      "label=0, text=$FTI - TechnipFMC downgraded at Berenberg but called Top Pick at Deutsche Bank https://t.co/XKcPDilIuU\n"
     ]
    }
   ],
   "source": [
    "for entry in dataset[\"train\"].select(range(6)):\n",
    "    text = entry[\"text\"]\n",
    "    label = entry[\"label\"]\n",
    "    print(f\"label={label}, text={text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load GPT-2 and tokenizer and evaluate on the test set\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# model name\n",
    "model_name = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9543/9543 [00:01<00:00, 5398.98 examples/s]\n",
      "Map: 100%|██████████| 2388/2388 [00:00<00:00, 6040.67 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 9543\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 2388\n",
       " })}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = {}\n",
    "splits = [\"train\", \"validation\"]\n",
    "\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c346c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 4093,\n",
       " 43,\n",
       " 720,\n",
       " 49,\n",
       " 5097,\n",
       " 532,\n",
       " 21198,\n",
       " 5330,\n",
       " 2173,\n",
       " 284,\n",
       " 1492,\n",
       " 654,\n",
       " 10453,\n",
       " 379,\n",
       " 40886,\n",
       " 290,\n",
       " 8111,\n",
       " 18020,\n",
       " 3740,\n",
       " 1378,\n",
       " 83,\n",
       " 13,\n",
       " 1073,\n",
       " 14,\n",
       " 88,\n",
       " 38,\n",
       " 34523,\n",
       " 51,\n",
       " 17,\n",
       " 3041,\n",
       " 35,\n",
       " 18,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][1][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3425fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for pytorch\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543b361",
   "metadata": {},
   "source": [
    "## Load and Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebc0636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=3,\n",
    "    id2label={0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"},\n",
    "    label2id={\"Bearish\": 0, \"Bullish\": 1, \"Neutral\": 2 }\n",
    ")\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5f3488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Freeze all the parameter of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a293ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=3, bias=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142a325",
   "metadata": {},
   "source": [
    "## Train the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d93c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2386 [05:36<?, ?it/s]\n",
      "  0%|          | 0/2386 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "## TODO: more classification metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "# Ensure the model's config recognizes the padding token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# Use the HuggingFace Trainer class to handle the training and eval loop \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_gpu_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "768f2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (2.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (0.24.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch>=1.10.0->accelerate)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/lightweightFT/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: sympy, accelerate\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed accelerate-1.0.1 sympy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bed05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67f1d7e1",
   "metadata": {},
   "source": [
    "## Let's try fine-tuning the gpt2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f265d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6cbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ff2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578670b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightweightFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
